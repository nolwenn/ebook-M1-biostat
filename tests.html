<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Inference and statistical tests | Little e-book for MPH1 biostatistics</title>
  <meta name="description" content="This is a little book of essential biostatistic concepts for Master 1 in Public Health." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Inference and statistical tests | Little e-book for MPH1 biostatistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a little book of essential biostatistic concepts for Master 1 in Public Health." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Inference and statistical tests | Little e-book for MPH1 biostatistics" />
  
  <meta name="twitter:description" content="This is a little book of essential biostatistic concepts for Master 1 in Public Health." />
  

<meta name="author" content="Nolwenn Le Meur, PhD - EHESP associate professor in Biostatistics and Bioinformatic" />


<meta name="date" content="2022-12-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inferencestat.html"/>
<link rel="next" href="introduction-to-regression-modelling.html"/>
<script src="libs/header-attrs-2.12/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li><a href="index.html#prerequisites">Prerequisites<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#lecture-tips"><i class="fa fa-check"></i><b>1.1</b> Lecture Tips<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variables.html"><a href="variables.html"><i class="fa fa-check"></i><b>2</b> Data: Statistical units and Variables<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="variables.html"><a href="variables.html#statistical-units"><i class="fa fa-check"></i><b>2.1</b> Statistical units<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="variables.html"><a href="variables.html#variables-1"><i class="fa fa-check"></i><b>2.2</b> Variables<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="variables.html"><a href="variables.html#data-storage"><i class="fa fa-check"></i><b>2.3</b> Data storage<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="variables.html"><a href="variables.html#variable-types"><i class="fa fa-check"></i><b>2.4</b> Variable types<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statdesc.html"><a href="statdesc.html"><i class="fa fa-check"></i><b>3</b> Descriptive statistics<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="statdesc.html"><a href="statdesc.html#frequency-table"><i class="fa fa-check"></i><b>3.1</b> Frequency table<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="statdesc.html"><a href="statdesc.html#central-parameters"><i class="fa fa-check"></i><b>3.2</b> Central parameters<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="statdesc.html"><a href="statdesc.html#mean"><i class="fa fa-check"></i><b>3.2.1</b> Mean<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="statdesc.html"><a href="statdesc.html#median"><i class="fa fa-check"></i><b>3.2.2</b> Median<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="statdesc.html"><a href="statdesc.html#percentile-and-quantile"><i class="fa fa-check"></i><b>3.2.3</b> Percentile and quantile<span></span></a></li>
<li class="chapter" data-level="3.2.4" data-path="statdesc.html"><a href="statdesc.html#mode"><i class="fa fa-check"></i><b>3.2.4</b> Mode<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statdesc.html"><a href="statdesc.html#variation-parameters"><i class="fa fa-check"></i><b>3.3</b> Variation parameters<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="statdesc.html"><a href="statdesc.html#range-and-iqr"><i class="fa fa-check"></i><b>3.3.1</b> Range and IQR<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="statdesc.html"><a href="statdesc.html#sd"><i class="fa fa-check"></i><b>3.3.2</b> Variance and standard deviation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="statdesc.html"><a href="statdesc.html#plot"><i class="fa fa-check"></i><b>3.4</b> Graphical summary<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="statdesc.html"><a href="statdesc.html#barplot"><i class="fa fa-check"></i><b>3.4.1</b> Barplot<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="statdesc.html"><a href="statdesc.html#pie-chart"><i class="fa fa-check"></i><b>3.4.2</b> Pie chart<span></span></a></li>
<li class="chapter" data-level="3.4.3" data-path="statdesc.html"><a href="statdesc.html#histogram"><i class="fa fa-check"></i><b>3.4.3</b> Histogram<span></span></a></li>
<li class="chapter" data-level="3.4.4" data-path="statdesc.html"><a href="statdesc.html#boxplot"><i class="fa fa-check"></i><b>3.4.4</b> Boxplot<span></span></a></li>
<li class="chapter" data-level="3.4.5" data-path="statdesc.html"><a href="statdesc.html#scatterplot"><i class="fa fa-check"></i><b>3.4.5</b> Scatterplot<span></span></a></li>
<li class="chapter" data-level="3.4.6" data-path="statdesc.html"><a href="statdesc.html#communication-tips"><i class="fa fa-check"></i><b>3.4.6</b> Communication tips<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferencestat.html"><a href="inferencestat.html"><i class="fa fa-check"></i><b>4</b> Inference and sample<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="inferencestat.html"><a href="inferencestat.html#sample"><i class="fa fa-check"></i><b>4.1</b> Sample<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inferencestat.html"><a href="inferencestat.html#population-versus-sample"><i class="fa fa-check"></i><b>4.1.1</b> Population versus Sample<span></span></a></li>
<li class="chapter" data-level="4.1.2" data-path="inferencestat.html"><a href="inferencestat.html#sample-designs"><i class="fa fa-check"></i><b>4.1.2</b> Sample designs<span></span></a></li>
<li class="chapter" data-level="4.1.3" data-path="inferencestat.html"><a href="inferencestat.html#probability-sampling"><i class="fa fa-check"></i><b>4.1.3</b> Probability sampling<span></span></a></li>
<li class="chapter" data-level="4.1.4" data-path="inferencestat.html"><a href="inferencestat.html#non-probability-sampling"><i class="fa fa-check"></i><b>4.1.4</b> Non-probability sampling<span></span></a></li>
<li class="chapter" data-level="4.1.5" data-path="inferencestat.html"><a href="inferencestat.html#sampling-bias"><i class="fa fa-check"></i><b>4.1.5</b> Sampling bias<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inferencestat.html"><a href="inferencestat.html#confidence-intervals"><i class="fa fa-check"></i><b>4.2</b> Confidence intervals<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inferencestat.html"><a href="inferencestat.html#within-and-between-sample-variation"><i class="fa fa-check"></i><b>4.2.1</b> Within and between sample variation<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="inferencestat.html"><a href="inferencestat.html#the-clt-and-the-confidence-interval"><i class="fa fa-check"></i><b>4.2.2</b> The CLT and the confidence interval<span></span></a></li>
<li class="chapter" data-level="4.2.3" data-path="inferencestat.html"><a href="inferencestat.html#interpretation-of-confidence-intervals"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation of confidence intervals<span></span></a></li>
<li class="chapter" data-level="4.2.4" data-path="inferencestat.html"><a href="inferencestat.html#why-1.96"><i class="fa fa-check"></i><b>4.2.4</b> Why 1.96?<span></span></a></li>
<li class="chapter" data-level="4.2.5" data-path="inferencestat.html"><a href="inferencestat.html#precision-or-margin-error"><i class="fa fa-check"></i><b>4.2.5</b> Precision or Margin error<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tests.html"><a href="tests.html"><i class="fa fa-check"></i><b>5</b> Inference and statistical tests<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="tests.html"><a href="tests.html#formulate-a-hypothesis"><i class="fa fa-check"></i><b>5.1</b> Formulate a hypothesis<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="tests.html"><a href="tests.html#comparison-of-two-means"><i class="fa fa-check"></i><b>5.2</b> Comparison of two means<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="tests.html"><a href="tests.html#comparison-of-two-proportions"><i class="fa fa-check"></i><b>5.3</b> Comparison of two proportions<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="tests.html"><a href="tests.html#chi-square-test"><i class="fa fa-check"></i><b>5.3.1</b> Chi-square test<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="tests.html"><a href="tests.html#fishers-exact-test"><i class="fa fa-check"></i><b>5.3.2</b> Fisher’s Exact test<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tests.html"><a href="tests.html#alpha-p"><i class="fa fa-check"></i><b>5.4</b> Risk <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(p-value\)</span><span></span></a></li>
<li class="chapter" data-level="5.5" data-path="tests.html"><a href="tests.html#risk-alpha-and-risk-beta"><i class="fa fa-check"></i><b>5.5</b> Risk <span class="math inline">\(\alpha\)</span> and risk <span class="math inline">\(\beta\)</span><span></span></a></li>
<li class="chapter" data-level="5.6" data-path="tests.html"><a href="tests.html#multi-comp"><i class="fa fa-check"></i><b>5.6</b> Comparison of multiple groups<span></span></a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="tests.html"><a href="tests.html#graphical-comparison"><i class="fa fa-check"></i><b>5.6.1</b> Graphical comparison<span></span></a></li>
<li class="chapter" data-level="5.6.2" data-path="tests.html"><a href="tests.html#analysis-of-variance"><i class="fa fa-check"></i><b>5.6.2</b> Analysis Of Variance<span></span></a></li>
<li class="chapter" data-level="5.6.3" data-path="tests.html"><a href="tests.html#post-hoc-analysis-and-anova-assumptions"><i class="fa fa-check"></i><b>5.6.3</b> Post-hoc analysis and ANOVA assumptions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="tests.html"><a href="tests.html#paranonpara"><i class="fa fa-check"></i><b>5.7</b> Parametric and non-parametric test<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="tests.html"><a href="tests.html#asessing-normality"><i class="fa fa-check"></i><b>5.7.1</b> Asessing Normality<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="tests.html"><a href="tests.html#two-sample-wilcoxon-test-or-mann-whitney-u-test"><i class="fa fa-check"></i><b>5.7.2</b> Two-sample Wilcoxon test (or Mann-Whitney U test)<span></span></a></li>
<li class="chapter" data-level="5.7.3" data-path="tests.html"><a href="tests.html#which-test-to-use"><i class="fa fa-check"></i><b>5.7.3</b> Which test to use?<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html"><i class="fa fa-check"></i><b>6</b> Introduction to regression modelling<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#simplelm"><i class="fa fa-check"></i><b>6.1</b> Simple linear regression<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#pearsons-coefficient-of-correlation"><i class="fa fa-check"></i><b>6.1.1</b> Pearson’s coefficient of correlation<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>6.1.2</b> Simple linear regression model<span></span></a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#posthocreg"><i class="fa fa-check"></i><b>6.1.3</b> Post-hoc assumptions verification<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#multiple-linear-regression-model"><i class="fa fa-check"></i><b>6.2</b> Multiple linear regression model<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#logistic-regression-model"><i class="fa fa-check"></i><b>6.3</b> Logistic regression model<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#collinearity"><i class="fa fa-check"></i><b>6.4</b> Collinearity<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#detecting-multi-collinearity"><i class="fa fa-check"></i><b>6.5</b> Detecting (multi-)collinearity<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#coefficient-of-correlation-and-visual-assessment"><i class="fa fa-check"></i><b>6.5.1</b> Coefficient of correlation and visual assessment<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#variance-inflation-factor"><i class="fa fa-check"></i><b>6.5.2</b> Variance Inflation Factor<span></span></a></li>
<li class="chapter" data-level="6.5.3" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#remedial-measures"><i class="fa fa-check"></i><b>6.5.3</b> Remedial measures<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#explanatory-variable-selection"><i class="fa fa-check"></i><b>6.6</b> Explanatory variable selection<span></span></a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#iterative-procedures-for-explanatory-variable-selection"><i class="fa fa-check"></i><b>6.6.1</b> Iterative procedures for explanatory variable selection<span></span></a></li>
<li class="chapter" data-level="6.6.2" data-path="introduction-to-regression-modelling.html"><a href="introduction-to-regression-modelling.html#goodness-of-fit-analysis"><i class="fa fa-check"></i><b>6.6.2</b> Goodness of fit analysis<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>7</b> Glossary<span></span></a></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Little e-book for MPH1 biostatistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tests" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Inference and statistical tests<a href="tests.html#tests" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="objective">
<ul>
<li>Understand statistical test reasoning</li>
<li>Interpret results of a statistical test</li>
<li>Discuss significance of a statistical test</li>
</ul>
</div>
<p>The aim of a statistical test is to reach a scientific decision on a difference (or effect), on a
probabilistic basis, based on observed data.</p>
<p>When assessing differences between groups (<em>Who</em>), you have to define <em>What</em> to compare. According to the type of the variable, you will choose a statistical parameter (mean, proportion, data distribution) to perform the comparison. The comparison will be based on hypothesis, with possible assumption to verify, and the associated statistical test.</p>
<p>In summary, the procedure is as follow:</p>
<ol style="list-style-type: decimal">
<li>Formulate hypothesis to be tested</li>
<li>Choose the appropriate statistical test</li>
<li>Calculate the appropriate statistic measure</li>
<li>Interpret the result</li>
</ol>
<div id="formulate-a-hypothesis" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Formulate a hypothesis<a href="tests.html#formulate-a-hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In <strong>hypothesis formulation</strong> you always have two possibilities: <strong>it is not different OR it is different</strong>.</p>
<p>In the HBSC data, we are interested in the characteristics of the smoking students compare to the non-smoking. Do they differ by some characteristics?</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 5.1  </strong></span>We would like to test whether there is, on average, a difference in height between smokers and non-smokers.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 5.2  </strong></span>We would like to test if the proportion of smokers varies by gender.</p>
</div>
<p>First, we describe the distribution of the variable between the groups.</p>
<p>In example 5.1, the height is a quantitative continuous variable which can be summarized by the mean (Table <a href="tests.html#tab:smokeheight">5.1</a>). In our sample, the students who do not smoke measure on average 157 cm while the students who smoke measure in average 166 cm. The question is “At the population level, is that different knowing that you do have individual variation (SD) and sample variations (SE)?”</p>
<table>
<caption>
<span id="tab:smokeheight">Table 5.1: </span>Description of the Height (cm) variable by smoking group (0=non-smoking, 1=smoking) in the French HBSC database in 2006.
</caption>
<thead>
<tr>
<th style="text-align:right;">
SmokingStatus
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
SD
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
Q1
</th>
<th style="text-align:right;">
Q3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
157.89
</td>
<td style="text-align:right;">
12.00
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
149
</td>
<td style="text-align:right;">
166.25
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
166.26
</td>
<td style="text-align:right;">
11.39
</td>
<td style="text-align:right;">
168
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
173.00
</td>
</tr>
</tbody>
</table>
<p>In example 5.2, the gender is a qualitative variable which can be summarized into proportions (Table <a href="tests.html#tab:smokegender">5.2</a>). In our sample, the proportion of students <em>seem</em> to different between groups. The question is “At the population level, are those proportions real different knowing that you do have individuals’ variation (<em>sd</em>) and sampling variation (<em>se</em>)?”</p>
<table>
<caption>
<span id="tab:smokegender">Table 5.2: </span>Proportion of students smoking (1) or non-smoking (0) by gender in the French HBSC database in 2006.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Smoking status
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Gender
</th>
<th style="text-align:right;">
No
</th>
<th style="text-align:right;">
Yes
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boy
</td>
<td style="text-align:right;">
85.26
</td>
<td style="text-align:right;">
14.74
</td>
</tr>
<tr>
<td style="text-align:left;">
girl
</td>
<td style="text-align:right;">
87.90
</td>
<td style="text-align:right;">
12.10
</td>
</tr>
</tbody>
</table>
<p><strong>In theory</strong>, we test whether the two groups (samples) come from the same population (Figure <a href="tests.html#fig:meancomparison">5.1</a>). For instance, sample 1 with mean <em>m1</em> from population 1 with <span class="math inline">\(\mu_1\)</span> is coming from the same population as sample 2 with mean <em>m2</em> from population 1 with <span class="math inline">\(\mu_2\)</span>. Population 1 is equal to population 2.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:meancomparison"></span>
<img src="fig/meancomparison.png" alt="Population versus Sample" width="70%" />
<p class="caption">
Figure 5.1: Population versus Sample
</p>
</div>
<p>When you state the hypothesis you should explicit H0 an H1.</p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 5.1  </strong></span><strong>H0</strong> : The null hypothesis is that the parameters are EQUAL, i.e they are not different.</p>
<p><strong>H1</strong>: The alternative hypothesis is that the parameters are NOT EQUAL, i.e they are different.</p>
</div>
<p>I like to write down the hypothesis with the <span class="math inline">\(=\)</span> sign and <span class="math inline">\(\neq\)</span> as I find easier to pick the test and interpret the results afterward.</p>
<p>Remember the <strong>W’s</strong> they should appear in the hypothesis if you have the information.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 5.3  </strong></span>We would like to test whether there is, on average, a difference in height between smokers and non-smokers.</p>
<p>H0: In France in 2006, the mean height of the students 11-16 who smoke was equal to the mean height of the students 11-16 who do not smoke.</p>
<p>H1: In France in 2006, the mean height of the students 11-16 who smoke was NOT equal to the mean height of the students 11-16 who do not smoke.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 5.4  </strong></span>We would like to test if the proportion of smokers varies according by gender.</p>
<p>H0: In France in 2006, the proportion of student girls aged 11-16 in the smoker group was equal to the proportion of student girls aged 11-16 in the non-smoker group</p>
<p>H1: In France in 2006, the proportion of student girls aged 11-16 in the smoker group was NOT equal to the proportion of student girls aged 11-16 in the non-smoker group</p>
</div>
<p>A statistical test is always performed to answer the H0 hypothesis (Figure <a href="tests.html#fig:testH0rule">5.2</a>).</p>
<ul>
<li>When we prove that the statistical parameters differ we reject H0 and accept H1. We say that we observed a statistically significant difference between the parameters.</li>
<li>When we cannot prove that the statistical parameters differ, we stay under H0 and say that: <strong>we fail to reject H0</strong> because we cannot show any statistically significant difference. H0 is never accepted as an error risk still exists that we can not compute.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:testH0rule"></span>
<img src="fig/testH0rule.png" alt="Statistical test interpretation" width="70%" />
<p class="caption">
Figure 5.2: Statistical test interpretation
</p>
</div>
</div>
<div id="comparison-of-two-means" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Comparison of two means<a href="tests.html#comparison-of-two-means" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Once the hypothesis are stated, we choose a test. In the context if the comparison of means, the test will assess whether the observed difference (<span class="math inline">\(\Delta\)</span>) between the two groups is random (due to individuals’ and sampling variations) or not (Figure <a href="tests.html#fig:meandelta">5.3</a>).</p>
<p>In theory, if H0 is true <span class="math inline">\(\Delta = m_1 - m_2 = 0\)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:meandelta"></span>
<img src="fig/meandelta.png" alt="Population versus Sample" width="70%" />
<p class="caption">
Figure 5.3: Population versus Sample
</p>
</div>
<p>But we never compare <span class="math inline">\(\Delta\)</span> to 0 as we need to take into account the individuals’ fluctuation (<em>sd</em>) and sampling (<em>se</em>) variation. What is the critical value, then ? Can you guess ?</p>
<p>The critical value depends on the risk you are willing to take to conclude about a difference
that does not exist in reality, the <span class="math inline">\(\alpha\)</span> risk.</p>
<p>When comparing means, you make the assumption that your sample’s distributions are not too different from a Normal distribution. Therefore, the difference <span class="math inline">\(\Delta=m_1-m_2\)</span> should follow a Normal distribution centered on 0. Then to take into account the individuals and sampling variation, the difference is standardized. The statistical value <span class="math inline">\((m_1-m_2)/s_\Delta\)</span> is computed and compare to the critical value <span class="math inline">\(Z_\alpha\)</span> of the centered reduced Normal distribution for a risk <span class="math inline">\(\alpha\)</span>. [Note <span class="math inline">\(s_\Delta\)</span> is function of the chosen test]</p>
<p>For a risk <span class="math inline">\(\alpha=5\)</span>% the critical value is <span class="math inline">\(Z_\alpha=1.96\)</span>.</p>
<p>If H0 is true, 95% of values of <span class="math inline">\((m_1-m_2)/s_\Delta\)</span> are between -1.96 and 1.96. If the statistical value <span class="math inline">\((m_1-m_2)/s_\Delta\)</span> returned by your test is above 1.96 or below -1.96, you reject H0 and accept H1.</p>
<div class="practice">
<p>In example 1, the statistical value for a risk <span class="math inline">\(\alpha=0.05\)</span> is -5.4582. What is you conclusion?</p>
</div>
</div>
<div id="comparison-of-two-proportions" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Comparison of two proportions<a href="tests.html#comparison-of-two-proportions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the context of proportion comparison like in example 2, the objective is to assess whether the proportion of cases among the exposed group is equal to the proportion of cases among the non-exposed group.</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 5.5  </strong></span>We would like to test if the proportion of smokers varies by gender.</p>
<p>H0: In France in 2006, the proportion of student boys aged 11-16 in the smoker group was equal to the proportion of student girls aged 11-16 in the smoker group</p>
<p>H1: In France in 2006, the proportion of student boys aged 11-16 in the smoker group was NOT equal to the proportion of student girls aged 11-16 in the smoker group</p>
</div>
<p>To this aim, we will compute the standardized differences (distances) between groups.</p>
<div id="chi-square-test" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Chi-square test<a href="tests.html#chi-square-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Chi-square is often the test used as rather intuitive and non computer greedy.</p>
<p>First, we compute a two-way table with your <code>observed counts</code>:</p>
<table>
<caption>
<span id="tab:chisquare1">Table 5.3: </span>Observed number of boys and girls students smoking or not in the French HBSC database in 2006.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Smoking status
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Gender
</th>
<th style="text-align:right;">
No
</th>
<th style="text-align:right;">
Yes
</th>
<th style="text-align:right;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boy
</td>
<td style="text-align:right;">
214
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
251
</td>
</tr>
<tr>
<td style="text-align:left;">
girl
</td>
<td style="text-align:right;">
218
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
248
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:right;">
432
</td>
<td style="text-align:right;">
67
</td>
<td style="text-align:right;">
499
</td>
</tr>
</tbody>
</table>
<p>Under H0 the absence of difference (independence assumption), we would expect to have the same proportion of smokers among the girls and the boys. So keeping the total margins what would be the expected counts? (Table <a href="tests.html#tab:chisquare2">5.4</a>)</p>
<table>
<caption>
<span id="tab:chisquare2">Table 5.4: </span>Expected number of boys and girls smoking or not in the French HBSC database in 2006.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Smoking status
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
No
</th>
<th style="text-align:left;">
Yes
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boy
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
251
</td>
</tr>
<tr>
<td style="text-align:left;">
girl
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
248
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
432
</td>
<td style="text-align:left;">
67
</td>
<td style="text-align:left;">
499
</td>
</tr>
</tbody>
</table>
<p>To compute a theoretical two-way table with your <code>expected counts</code>:</p>
<ul>
<li>the proportion exposed students is: <span class="math inline">\(p= 67/499= 0.134\)</span>, <em>i.e</em> 13.4%.</li>
<li>the number of boys students exposed would be: <span class="math inline">\(p= (67/499)*251 = 33.7\)</span>.</li>
</ul>
<table>
<caption>
<span id="tab:chisquare3">Table 5.5: </span>Expected number of boys and girls smoking or not in the French HBSC database in 2006.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Smoking status
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
No
</th>
<th style="text-align:left;">
Yes
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boy
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
(67/499)*251
</td>
<td style="text-align:left;">
251
</td>
</tr>
<tr>
<td style="text-align:left;">
girl
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
?
</td>
<td style="text-align:left;">
248
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
432
</td>
<td style="text-align:left;">
67
</td>
<td style="text-align:left;">
499
</td>
</tr>
</tbody>
</table>
<table>
<caption>
<span id="tab:chisquare4">Table 5.6: </span>Expected number of boys and girls students smoking or not in the French HBSC database in 2006.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Smoking status
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
No
</th>
<th style="text-align:left;">
Yes
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boy
</td>
<td style="text-align:left;">
217.3
</td>
<td style="text-align:left;">
33.7
</td>
<td style="text-align:left;">
251
</td>
</tr>
<tr>
<td style="text-align:left;">
girl
</td>
<td style="text-align:left;">
214.7
</td>
<td style="text-align:left;">
33.3
</td>
<td style="text-align:left;">
248
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
432
</td>
<td style="text-align:left;">
67
</td>
<td style="text-align:left;">
499
</td>
</tr>
</tbody>
</table>
<p>Then we compute the Chi-square (<span class="math inline">\(\chi^2\)</span>) statistic which is the standardized sum of the differences between the observed and the expected values.</p>
<div class="center">
<p><span class="math inline">\(\chi^2_{Obs}= \sum(\dfrac{(Obs-Exp)^2}{Exp}\)</span></p>
</div>
<p>Using the R statistical software, we have</p>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  hbsc$Gender and hbsc$SmokingStatus
## X-squared = 0.54013, df = 1, p-value = 0.4624</code></pre>
<p>The <span class="math inline">\(\chi^2_{Obs} = 0.54\)</span>. Now the question is:</p>
<p>“Is <span class="math inline">\(\chi^2_{Obs}\)</span> equal to 0?”</p>
<p>As for the comparison of means, in theory, if H0 is true <span class="math inline">\(\chi^2_{Obs} \sim 0\)</span> but it is never 0. There are variations and we test H0 with a <span class="math inline">\(\alpha\)</span> risk. Therefore, what is the threshold?</p>
<p>To define that threshold, we need to choose the correct statistical law. The <span class="math inline">\(\chi^2\)</span> distribution depends on <em>k</em>, the number of degrees of freedom (df) which depends on the number of characteristics of the two variables we are comparing (Figure <a href="tests.html#fig:chi2-distrib">5.4</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chi2-distrib"></span>
<img src="fig/chi2-distrib.png" alt="Chi2 distribution depends on the degree of fredom K" width="70%" />
<p class="caption">
Figure 5.4: Chi2 distribution depends on the degree of fredom K
</p>
</div>
<p>When the two-way table of the expected number is drawn, the degree of freedom is the number of values in the final calculation that are free to vary. For instance, in a 2x2 table once you have set one value the others cannot change. The quick formula to compute the degree of freedom (df) for the <span class="math inline">\(\chi^2\)</span> distribution is the number of rows in the table minus 1 multiply by the number of columns in the table minus 1:</p>
<div class="center">
<p>df = (#rows - 1) * (#cols - 1)</p>
</div>
<div class="practice">
<p>For a 2x3 table, what is the degree of freedom?</p>
</div>
<p>Next, we need to look at the statistical table of the <span class="math inline">\(\chi^2\)</span> law (Figure <a href="tests.html#fig:chi2-table">5.5</a>). The threshold value also depends on the <span class="math inline">\(\alpha\)</span> risk you are willing to take.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chi2-table"></span>
<img src="fig/chi2-table.png" alt="Chi2 distribution depends on the degree of fredom K" width="70%" />
<p class="caption">
Figure 5.5: Chi2 distribution depends on the degree of fredom K
</p>
</div>
<p>In the <span class="math inline">\(\chi^2\)</span> table, for our 2x2 table the <span class="math inline">\(df = 1\)</span> and for a <span class="math inline">\(\alpha\)</span> risk of 5%, the <span class="math inline">\(\chi^2_{Theo}= 3.84\)</span>.</p>
<p>Next the decision rule using the p-value is the same as for the comparison of means.</p>
<div class="practice">
<p>In the example, the statistical value for a risk <span class="math inline">\(\alpha=0.05\)</span> and <span class="math inline">\(df=1\)</span> is 3.84. The <span class="math inline">\(\chi^2_{Obs} = 0.54\)</span>. What is you conclusion?</p>
</div>
<div class="caution">
<p>To correctly use the <span class="math inline">\(\chi^2\)</span> test and have accurate estimation of the associated probabilities, we need to have at least <strong>n=5</strong> count in each cell of the <strong>table of expected</strong> numbers.</p>
</div>
</div>
<div id="fishers-exact-test" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Fisher’s Exact test<a href="tests.html#fishers-exact-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To compare proportions the Fisher’s Exact test is even better than <span class="math inline">\(\chi^2\)</span> as it computes the exact probability of obtaining a difference even greater if H0 is true. However the formula is more complex and difficult to compute by hand . A computer is highly recommended (Table <a href="tests.html#tab:FisherTab">5.7</a>).</p>
<table>
<caption>
<span id="tab:FisherTab">Table 5.7: </span>Fisher Exact test principal.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Smoking status
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
No
</th>
<th style="text-align:left;">
Yes
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
boy
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:left;">
c
</td>
<td style="text-align:left;">
n1
</td>
</tr>
<tr>
<td style="text-align:left;">
girl
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
d
</td>
<td style="text-align:left;">
n2
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
t1
</td>
<td style="text-align:left;">
t2
</td>
<td style="text-align:left;">
N
</td>
</tr>
</tbody>
</table>
<div class="center">
<p><span class="math inline">\(p = \dfrac{n_1!n_2!t_1!t_2!}{a!b!c!d!}\)</span></p>
</div>
<p>Using the R statistical software, we have</p>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  hbsc$Gender and hbsc$SmokingStatus
## p-value = 0.4316
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.4571282 1.3779096
## sample estimates:
## odds ratio 
##  0.7962999</code></pre>
<p>How to read the output of the Fisher’s Exact test?</p>
<ol style="list-style-type: decimal">
<li>We can look at the odds ratio and the confidence interval (95% CI)</li>
</ol>
<p>As it is a ratio (numerator/denominator) if there is no difference (numerator=denominator) the <span class="math inline">\(odds ratio \sim 1\)</span>. However it is never 1, we need to look at the confidence interval (at a certain risk level) to conclude.</p>
<ul>
<li>If the confidence interval includes 1, we fail to reject H0, we can not conclude that the proportions differ.</li>
<li>If the confidence interval does not include 1, we reject H0, the proportions differ.</li>
</ul>
<div class="practice">
<p>In the example, the Fisher’s exact test returns an odds ratio of 0.79 and 95% CI [0.45, 1.37]. What is your conclusion?</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>We can look at the <span class="math inline">\(p-value\)</span> but what is a <span class="math inline">\(p-value\)</span> ? See next section <a href="tests.html#alpha-p">5.4</a>.</li>
</ol>
</div>
</div>
<div id="alpha-p" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Risk <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(p-value\)</span><a href="tests.html#alpha-p" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The demonstrations above and the use of <span class="math inline">\(Z_\alpha\)</span> or <span class="math inline">\(\chi^2_\alpha\)</span> values are valid for comparison of means or proportions under some assumptions and conditions (<span class="math inline">\(Z_\alpha\)</span> with sample size above 30 in each group; <span class="math inline">\(\chi^2_\alpha\)</span> function <em>df</em>…). What is happening for other tests?</p>
<p>The philosophy is exactly the same but the critical value might come from other statistical laws than the Normal law (Chi-square, Binomial, Poisson…). It might be difficult to retrieve the critical value need to compare to your computed statistical value. There are more statistical tables that there are statistical tests.</p>
<p>The common practice is then to compare the risk <span class="math inline">\(\alpha\)</span>, defined <em>a priori</em>, to the <span class="math inline">\(p-value\)</span> returned by the test <em>a posteriori</em>.We want to know the ultimate risk that is taken. Meaning, the risk corresponding to the value found by the test.</p>
<div class="define">
<p><strong><span class="math inline">\(\alpha\)</span> risk and <span class="math inline">\(p-value\)</span> </strong></p>
<p><span class="math inline">\(\alpha\)</span> risk: <em>a priori</em> risk to conclude about a difference that does not exist in reality</p>
<p><span class="math inline">\(p-value\)</span>: <em>a posteriori</em> error risk that is taken knowing the result of the test</p>
</div>
<div class="define">
<p><strong>Statistical test’s decision rule</strong></p>
<p>When <span class="math inline">\(p-value\)</span> &gt; <span class="math inline">\(\alpha\)</span> risk, we FAIL to reject H0</p>
<p>When <span class="math inline">\(p-value \leq \alpha\)</span> risk, we reject H0</p>
</div>
<p>The <span class="math inline">\(p-value\)</span> is not synonymous of the importance of the possible difference between groups. In other words, a very small p-value means that the risk of making a mistake is very low. It does not mean that there is a huge difference between groups.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 5.6  </strong></span><strong>Example 3:</strong> Using 2 studies we are assessing 2 new methods (A and B) for the prevention of surgical site infections (SSI) compared to a conventional method (0).</p>
<ul>
<li>In study A: method A shows 12% of SSI and the conventional method 24% of SSI. The test between A and 0 returns a <span class="math inline">\(p-value \leq 0.05\)</span>.</li>
<li>In study B: method B shows 12% of SSI and the conventional method 24% of SSI. The test between B and 0 returns a <span class="math inline">\(p-value \leq 0.001\)</span></li>
</ul>
<p>Is method B better than method A in preventing SSI?</p>
</div>
<p>In the example above, we cannot tell if method B is better than method A : we did not test A versus B. The only information we get is that method A is different from method 0 and that method B is different from method 0. In fact method A and B present the same level of SSI. They might not be different but to conclude (with a certain level of confidence) we need to do a test. (see section 5.7 for comparison of multiple groups)</p>
<div class="practice">
<p>In example 1, we tested H0 with the Student’s T test. The <span class="math inline">\(p-value\)</span> is 4.342e-07. What is your conclusion?</p>
</div>
<div class="practice">
<p>In example 2, we tested H0 with a <span class="math inline">\(\chi^2\)</span> test. The <span class="math inline">\(p-value\)</span> is 0.462. What is your conclusion?</p>
<p>We also tested H0 with the Fisher’s Exact test. The <span class="math inline">\(p-value\)</span> is 0.431. Do you reach the same conclusion than with the <span class="math inline">\(\chi^2\)</span> test? Why?</p>
</div>
<p>If you try to compare the effects of different risk factors or compare similar studies but with different protocols, you sholud not compare the <em>p-values</em>. A <em>p-value</em> smaller than an other <em>p-value</em> does not mean that the observed difference is greater. It only means that your are more confident on the results of the test.</p>
<div class="caution">
<p>You should not compare p-values.</p>
</div>
<p><img src="fig/pval_nocomparison.png" width="80%" style="display: block; margin: auto;" /></p>
<p>In the cholestrol studies presented above, the <em>p-values</em> are different, one is smaller than the other (even significant). Although the observed differences in mean are identical. The difference is the same in study 1 and 2 (<span class="math inline">\(\delta=4\)</span>). The differences between the two studies are on the standard deviations of the samples and the sample sizes which affect the t-statistics.</p>
</div>
<div id="risk-alpha-and-risk-beta" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Risk <span class="math inline">\(\alpha\)</span> and risk <span class="math inline">\(\beta\)</span><a href="tests.html#risk-alpha-and-risk-beta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Why do we not accept H0?</strong></p>
<p>As mentioned earlier there is always a risk of being wrong (Figure <a href="tests.html#fig:riskdecision">5.6</a>) but that risk cannot be computed. It is <span class="math inline">\(\beta\)</span>.</p>
<p>In row the unknown truth, in column the conclusion of the test.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:riskdecision"></span>
<img src="fig/riskdecision.png" alt="Where do the risks stand?" width="50%" />
<p class="caption">
Figure 5.6: Where do the risks stand?
</p>
</div>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the probability of rejecting H0, when H0 is true (Figure <a href="tests.html#fig:alphavisual">5.7</a>)</li>
<li><span class="math inline">\(\beta\)</span> is the probability of failing to reject H0, when H1 is true (Figure <a href="tests.html#fig:betavisual">5.8</a>)</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:alphavisual"></span>
<img src="fig/alphavisual.png" alt="Population versus when it does not exist" width="40%" />
<p class="caption">
Figure 5.7: Population versus when it does not exist
</p>
</div>
<p>Imagine that you are the “eyes” and you are unable to see the perspective above the horizon. You will believe that the 2 lines make 1 and that they are of the same length but in reality the further away is longer. That is the <span class="math inline">\(\beta\)</span> risk.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betavisual"></span>
<img src="fig/betavisual.png" alt="Not seeing a difference when it does exist." width="40%" />
<p class="caption">
Figure 5.8: Not seeing a difference when it does exist.
</p>
</div>
<p><strong>What do you prefer <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> ?</strong></p>
<p>It is a difficult question.</p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 5.7  </strong></span><strong>Example 4:</strong> Hepatitis B vaccination and multiple sclerosis. Many study protocols have been conduct to assess an possible association between vaccination against Hepatitis B and the development of multiple sclerosis. The null hypothesis was that prevalence of multiple sclerosis was the same among people vaccinated against Hepatitis B and people not vaccinated against Hepatitis B.</p>
<p>In that context, what would you favour <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span>?</p>
</div>
<ul>
<li>Wrongly reject H0 and conclude that there is an effect : it could be catastrophic as the immunization coverage will fall down</li>
<li>Not seeing an effect: people may not get Hepatitis B vaccination but multiple sclerosis !!!</li>
</ul>
<p>Note that there have been many studies on the above question and that no effect as been seen so far.</p>
<div class="practice">
<p>You test the hypothesis of an absence of difference in school grades between gender. What is your conclusion if:</p>
<ul>
<li><p><span class="math inline">\(p-value = 0.049\)</span></p></li>
<li><p><span class="math inline">\(p-value = 0.051\)</span></p></li>
</ul>
<p>Comment on your conclusions.</p>
</div>
</div>
<div id="multi-comp" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Comparison of multiple groups<a href="tests.html#multi-comp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 5.8  </strong></span><strong>Example 5:</strong> We are interested in the effect of 2 treatments to gain weight. The protocol include treatment A, treatment B, and a placebo group. We would like to compare the weights between the 3 groups.</p>
<p>How do we compare the means? Can we do 2 by 2 comparisons?</p>
</div>
<p>Table 1: Effects of 2 treatments to gain weight</p>
<table>
<thead>
<tr class="header">
<th>Parameters</th>
<th>Treatment A</th>
<th>Treatment B</th>
<th>Placebo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sample size n</td>
<td>62</td>
<td>62</td>
<td>96</td>
</tr>
<tr class="even">
<td>mean weight</td>
<td>66.06</td>
<td>63.83</td>
<td>62.32</td>
</tr>
<tr class="odd">
<td>sd weight</td>
<td>3.18</td>
<td>2.16</td>
<td>3.12</td>
</tr>
</tbody>
</table>
<p>To the above question the answer is “No” : it increases the likelihood of incorrectly concluding that there are statistically significant differences, since each comparison adds to the probability of a type I error, <span class="math inline">\(\alpha\)</span>.</p>
<p>At the end, if <span class="math inline">\(k\)</span> is the number of comparisons, the error rate becomes <span class="math inline">\(1-(0.95)^k\)</span></p>
<div id="graphical-comparison" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Graphical comparison<a href="tests.html#graphical-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A boxplot and whisker plot (section <a href="statdesc.html#plot">3.4</a>) is an ideal graphical representation to compare data series of the same variable between different groups.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:boxplotanova"></span>
<img src="fig/boxplotanova-1.png" alt="Boxplot and whisker plot of the effect of different treatments on gain weight" width="60%" />
<p class="caption">
Figure 5.9: Boxplot and whisker plot of the effect of different treatments on gain weight
</p>
</div>
<p>Figure <a href="tests.html#fig:boxplotanova">5.9</a> shows that the distributions seems to differ. The median (black line with the box) are located at differ weight.</p>
<div class="practice">
<p>What are the IQR of the 3 groups?</p>
</div>
<p>In Figure <a href="tests.html#fig:boxplotanova">5.9</a> the medians are not in the middle of the box. This suggest that the distributions might be skewed.</p>
<p>Figure <a href="tests.html#fig:densityanova">5.10</a> presents density plots, similar to histograms, and reveals the same thing.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densityanova"></span>
<img src="fig/densityanova-1.png" alt="Density plots of the effect of different treatments on gain weight" width="60%" />
<p class="caption">
Figure 5.10: Density plots of the effect of different treatments on gain weight
</p>
</div>
<p>The next step is thus to statistically test the hypothesis of equality of means.</p>
</div>
<div id="analysis-of-variance" class="section level3 hasAnchor" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> Analysis Of Variance<a href="tests.html#analysis-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Analysis Of Variance or ANOVA allows comparing multiple groups</p>
<div class="define">
<p>The Analysis Of Variance or ANOVA systematically compare variability <em>within</em> and <em>between</em> groups. When the variations observed between groups is greater than the within group variation, at least one group is differ from the other.</p>
</div>
<p>The statistical hypotheses are:<br />
- H0: <span class="math inline">\(\mu1 = \mu2 = \mu3 ... = \mu k\)</span> with <span class="math inline">\(\alpha\)</span>=5%<br />
- H1: At least one mean is different from the other</p>
<p>where <span class="math inline">\(k\)</span> is the number of independent groups</p>
<p>To that aim we use the F-test (named in honor of Sir Ronald Fisher). The F-statistic is a ratio of two variances that examine variability.</p>
<div class="center">
<p><span class="math inline">\(F = \frac{Mean Square Between}{Mean Square Error}= \frac{MSB}{MSE}\)</span></p>
</div>
<ul>
<li>between groups being compared (Mean Square Between or Mean Square Treatment)</li>
<li>within the groups being compared (Mean Square Error or Mean Square Residuals)</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Mean Squares</th>
<th>Sums of Squares (SS)</th>
<th>DF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MSB = SSB/(k-1)</td>
<td><span class="math inline">\(SSB = \sum_{i=1}^k n_i(\bar{X_i}-\bar{X})^2\)</span></td>
<td>k-1</td>
</tr>
<tr class="even">
<td>MSE = SSE/(N-k)</td>
<td><span class="math inline">\(SSE = \sum_{i=1}^k\sum_{j=1}^n(X_{ij}-\bar{X_i})^2\)</span></td>
<td>N-k</td>
</tr>
<tr class="odd">
<td>total</td>
<td><span class="math inline">\(SST = \sum_{i=1}^k\sum_{j=1}^n(X_{ij}-\bar{X})^2\)</span></td>
<td>N-1</td>
</tr>
</tbody>
</table>
<p>where</p>
<p>DF = degree of freedom
<span class="math inline">\(X_{ij}\)</span> = individual observation <em>j</em> in treatment <em>i</em><br />
<span class="math inline">\(\bar{X_i}\)</span> = sample mean of the <span class="math inline">\(i^{th}\)</span> treatment (or group/sample)<br />
<span class="math inline">\(\bar{X}\)</span> = overall sample mean<br />
k = number of treatments or independent groups<br />
n = number of observations in treatment <em>i</em>
N = total number of observations or total sample size</p>
<p>In practice with R</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="tests.html#cb5-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">aov</span>(Weight <span class="sc">~</span> Treatment, <span class="at">data=</span>treatFrame)</span>
<span id="cb5-2"><a href="tests.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(res)</span></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Treatment     2  270.5  135.26   16.48 2.62e-07 ***
## Residuals   183 1501.7    8.21                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The F-value is the F statistics. The F-value = 16.48 (=135.26/8.21). To conclude under H0 and a given <span class="math inline">\(\alpha\)</span> risk, we can try to look for the appropriate statistical law and its associated table or we can conclude using the p-value.</p>
<div class="practice">
<p>The <span class="math inline">\(p-value = 2.62e^{-07}\)</span>. What is your conclusion?</p>
</div>
</div>
<div id="post-hoc-analysis-and-anova-assumptions" class="section level3 hasAnchor" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> Post-hoc analysis and ANOVA assumptions<a href="tests.html#post-hoc-analysis-and-anova-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The ANOVA results might help you conclude that a least one group varies differently that the others. However you will not know which group. To that aim you, need to perform a post-hoc analysis using the
TukeyHSD’s test.</p>
<p>In practice with R</p>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Weight ~ Treatment, data = treatFrame)
## 
## $Treatment
##          diff       lwr         upr     p adj
## B-A -1.667323 -2.883055 -0.45159037 0.0040296
## P-A -2.945509 -4.161241 -1.72977619 0.0000001
## P-B -1.278186 -2.493918 -0.06245338 0.0367906</code></pre>
<p>The output of the TukeyHSD’s test presents adjusted p-values (p adj) for the two groups comparisons. In addition is displayed the difference in means (diff) and the lower (lwr) and upper (upr) bounds of the 95% CI on the difference in means.</p>
<p>In example 5, Owing the TukeyHSD’s test, it seems that all means are differ from one another.</p>
<p>That is if we trust the appropriate use of the ANOVA and TukeyHSD’s tests…</p>
<p>The ANOVA analysis relies on several <strong>assumptions that need to be tested before</strong> computing the ANOVA. The ANOVA formula is a based on mean and variance that can only be used if the distributions are not too different from the Normal distribution. It is a parametric test (see section <a href="tests.html#paranonpara">5.7</a>). Before the ANOVA we need to verify with prior statistical tests that:</p>
<ol style="list-style-type: decimal">
<li><p>the outcome variable should be normally distributed within each group (<em>Shapiro test</em>)</p></li>
<li><p>the variance in each group should be similar (e.g. <em>Bartlett or Levene test</em>)</p></li>
<li><p>the observations are independent (not correlated or related to each other)</p></li>
</ol>
<p>However, the F-test is fairly resistant or robust to violations of assumptions 1 and 2.</p>
</div>
</div>
<div id="paranonpara" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Parametric and non-parametric test<a href="tests.html#paranonpara" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are “distribution-free” and, as such, can be used for non-Normal variables. Non-parametric tests are often based on the ranking of the values in the data serie.</p>
<p>For the non-parametric the hypothesis are:</p>
<p>H0: The two samples are from the same distribution</p>
<p>H1: one distribution is shifted in location higher or lower than the other</p>
<p>While for the parametric interested in comparing means the hypothesis are:</p>
<p>H0: The two samples have the same mean</p>
<p>H1: The two samples do not have the same mean</p>
<div id="asessing-normality" class="section level3 hasAnchor" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Asessing Normality<a href="tests.html#asessing-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Graphically</li>
</ol>
<p>Normality can be assess using an histogram or the cumulative distribution function.</p>
<p>In figure @ref(fig:normality_plotfunction), the data normally distributed are on the left where the histogram is symmetric and the cumulative distribution function has a S shape. On the right, the data not normally distributed present a skewed histogram and cumulative distribution function different from a S shape.</p>
<div class="figure" style="text-align: center">
<img src="fig/normality_plotfunction.png" alt="Assessing normality using graphics" width="40%" />
<p class="caption">
(#fig:distribution function)Assessing normality using graphics
</p>
</div>
<p>Note that in the skewed distribution, the mean will be largely different from the median and the mode parameters.</p>
<ol start="2" style="list-style-type: decimal">
<li>Statistically</li>
</ol>
<p>The Shapiro-Wilk test or Kolgomorov-Sminorv can be used do verify the normality a distribution. THe statistical hypothesis is:</p>
<p>H0: The sample distribution is equal to the Normal distribution</p>
<p>H1: The sample distribution is different from the Normal distribution</p>
<p>As an example, we randomly generate values drawn from a Normal distribution and test the Normality.</p>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  data
## W = 0.99103, p-value = 0.7475</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  data
## D = 0.094664, p-value = 0.3316
## alternative hypothesis: two-sided</code></pre>
<p>What is your conclusion?</p>
<p>Note that the advantage of the Kolgomorov-Smirnov’s test is that it can help assessing other type of distributions than the Normal distribution.</p>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  data
## D = 0.50342, p-value &lt; 2.2e-16
## alternative hypothesis: two-sided</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  data
## D = 0.050052, p-value = 0.9636
## alternative hypothesis: two-sided</code></pre>
<p>What are your conclusions?</p>
</div>
<div id="two-sample-wilcoxon-test-or-mann-whitney-u-test" class="section level3 hasAnchor" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> Two-sample Wilcoxon test (or Mann-Whitney U test)<a href="tests.html#two-sample-wilcoxon-test-or-mann-whitney-u-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Given two samples X et Y</li>
<li>Sort in increasing order the data from X and Y</li>
<li>Give a rank to the values</li>
<li>Compute the sum of rank for each sample R1 and R2</li>
<li>Compute the random variable Un1,n2 = min(Un1, Un2) and the associated Z statistics</li>
</ul>
<p><img src="fig/wilcoxon.png" width="60%" style="display: block; margin: auto;" /></p>
<p><img src="fig/zwilcoxon.png" width="40%" style="display: block; margin: auto;" /></p>
<p>If n1 and n2 &lt; 20, Mann-Whitney’s table</p>
<p>IF n1 and n2 &gt; 20, Normal law table</p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 5.9  </strong></span><strong>Example:</strong></p>
<p>Among diabetes patients, is there a difference in age at diagnosis between men and women?</p>
<ul>
<li>Women: 20 11 17 12</li>
<li>Men: 19 22 16 29 24</li>
</ul>
<p>Sorting data: 11 12 16 17 19 20 22 24 29</p>
<p>R1 = 1+2+4+6 = 13 and Un1 = 17</p>
<p>R2 = 3+5+7+8+9 = 32 and Un2= 3</p>
<p>Table de Mann-Whitney p-value =0.11</p>
<p>What is you conlusion?</p>
</div>
</div>
<div id="which-test-to-use" class="section level3 hasAnchor" number="5.7.3">
<h3><span class="header-section-number">5.7.3</span> Which test to use?<a href="tests.html#which-test-to-use" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Non-parametric tests are valid for both non-Normally distributed data and Normally distributed data, so why not use them all the time?</strong></p>
<p>When it is possible to perform both a parametric and a non-parametric test (because we have quantitative measurements) reducing the data to ranks and using the Wilcoxon/Mann-Whitney test will have about 95% of the power of a corresponding two-sample t-test. And, as we have seen,
often outliers are interesting in their own right. An analysis that simply ignores them
may miss an important fact or instance.</p>
<table>
<thead>
<tr class="header">
<th>Parametric</th>
<th>Non-Parametric equivalent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Paired t-test</td>
<td>Wilcoxon rank sum test*</td>
</tr>
<tr class="even">
<td>Unpaired t-test</td>
<td>Mann-Whitney U test*</td>
</tr>
<tr class="odd">
<td>Pearson correlation</td>
<td>Spearmann Correlation</td>
</tr>
<tr class="even">
<td>One analysis of variance</td>
<td>Kruskal Wallis test</td>
</tr>
</tbody>
</table>
<p>*In R statistical software, see Wilcoxon.test() and the arguments</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferencestat.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-to-regression-modelling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-TestStat.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
