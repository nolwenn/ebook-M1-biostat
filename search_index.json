[
["index.html", "Prerequisites", " Introduction to biostatistics for public health Master I Nolwenn Le Meur, PhD - EHESP associate professor 2020-10-09 Prerequisites Not to be afraid of numbers ! Otherwise I will try to work around it. "],
["intro.html", "Chapter 1 Introduction 1.1 Lecture Tips", " Chapter 1 Introduction You may wonder why you will need to learn some statistics as you do not plan to compute statistics yourself in your future job. But, as a public health professional, the decisions you may be brought to make based on data will be too important to delegate. You will want to be able to interpret the data that surrounds you and to come to your own conclusions (Sharpe, De Veaux, and Velleman 2012). At the end, I hope you will understand the importance of statistics in our complex world and even enjoy studying the discipline. 1.1 Lecture Tips When you see a term written like statistics in the text it is a concept that you must understand the definition and usage. A proper definition will often be highlighted in a box. No need to know the definition by heart. Different boxes will highlight the concepts, warnings and small practical exercises. This will be the definition of a concept you should understand along with its use This is a warning This is a small question you should try to answer This little book is being developed as a support for my online classroom in the context of the SARS-Cov2 pandemic. The content is a mix of my usual slides with definitions and examples from the books by Sharpe, De Veaux, and Velleman (2012) (second edition), by Diez, Barr, and Çetinkaya-Rundel (2019) (fourth edition) and by Ancelle (2017 in French). To illustrate the concepts, I mostly use a subset of the 2006 French Health Behavior School-aged Children database (HBSC). Since 1982 HBSC has been a pioneer cross-national study gaining insight into young people’s well-being, health behaviors and their social context. This research collaboration with the WHO Regional Office for Europe is conducted every four years in 50 countries and regions across Europe and North America. For more details please visit the HBSC website (Aarø et al. 1986) The statistical software R is used to illustrate some of the test and modeling examples (R Core Team 2020). Command and output examples are presented. R and the bookdown package were in fact used to create this little e-book (Xie 2016). References "],
["variables.html", "Chapter 2 Data and Variables 2.1 Statistical units 2.2 Data 2.3 Variable types", " Chapter 2 Data and Variables The W’s and their types are what you should be interested in and careful about in the data: Who What Where When Statistics, from the latin status and same root as State, is the art of counting and classifying. At first, statistics were used to describe countable information on population and rapidly they became essential to model and predict data from experience to foresee outcome and help decision making. 2.1 Statistical units “Statistics is all about variation”. In Public Health we are interested in population, composed of statistical units with characteristics that vary. For example it could be individuals with characteristics such as: age, height, weight, salary…(Figure 2.1) Note that your population of interest may also be the hospitals in your home country with characteristics such as the number of beds, nurses, doctors, patients…(Figure 2.2). The Who is the statistical unit, the unitary element of interest (individual, hospital, country…). Figure 2.1: Individuals as statistical units Figure 2.2: Hospitals as statistical units 2.2 Data The information on each statistical unit can be stored and displayed in a data table. Typically, the Who of the table are found in the leftmost column. The What are stored in the remaining columns. The What are variables, the recorded characteristics of the statistical units. Table 2.1 presents a snapshot of the HBSC data presented in the Introduction section (1). Table 2.1: A table of the first 10 rows and first 8 columns of the HBSC data, France 2006. ID Grade.level School.Status Gender Age Weight 5617 7th grade private boy 13.58 50.0 4578 6th grade public boy 12.00 47.7 6512 6th grade public girl 11.42 28.7 5695 10th grade private girl 15.58 48.5 3906 8th grade public boy 13.75 48.5 6266 6th grade public boy 11.67 59.8 363 9th grade public girl 15.25 NA 1095 6th grade public boy 12.75 47.5 5388 8th grade public girl 13.50 52.0 6730 8th grade public girl 15.25 58.0 Try to guess what these data represent and what information is available. Hint: Do not forget to read the title of the table. The Where and When are the context/location and time of the data collection. For instance in our HBSC the When is the year 2006 and the Where is in France. The scale of time and place are of great importance that need to be clearly defined. For example, the time can be a time point or a period of several months or years. The place as France could be metropolitan France (excluding island) or the whole France. We could also look at different geographical levels like the city, the county, the state… Those information have to be reported in every titles of every tables and plots you will create from the data along with the Who and What. A table or a figure should be self-explanatory (self-content). 2.3 Variable types Public Health data may come from various sources. For instance, they can be collected via interviews, surveys, or health information systems. In qualitative sciences, interviews are often based on open questions where answers are free text. We will not discuss that case in this class. In quantitative sciences, surveys or records from health information systems are based on short queries where short answers with a finite range of possibilities are expected. For instance, let’s say you are interested in tobacco consumption and plan a survey. You may ask the following questions with finite possibilities of answers: Variables are of different types (Figure 2.3). When a variable is allowed to takes a limited number of categorical values, or categories, and answers questions about how cases fall into those categories, we call it a categorical, or qualitative, variable. When a variable corresponds to measured numerical values with units and the variable tells us about the quantity of what is measured, we call it a quantitative variable (Sharpe, De Veaux, and Velleman 2012). The type of a variable will condition the statistical method chosen to summarize and describe your population of interest (Chapter 3). Figure 2.3: Different types of variables with example. The categorical, or qualitative, variables can be of two sub-types: the nominal and the ordinal variables. The nominal variables are for instance the colors of the eyes or various professions that count many categories. The nominal variables can also be binary with only two categories like smoking/non smoking or boys/girls. The ordinal variables take into account an ordering between the possible categories of the variables. For instance, a ordinal variable could be a scale of spiciness: neutral, middle, hot, very hot ! Note that this is suggestive. You can come up with a ranking where the intervals between the categories are not of equal width. The quantitative variables can be discrete or continuous. A quantitative discrete variable corresponds to numerical counts like the number of kids per household. A quantitative continuous variable corresponds to measures with potential decimals like the weight or height of pupils in the HBSC cohort. Propose a set of variables, one of each type. References "],
["statdesc.html", "Chapter 3 Descriptive statistics 3.1 Frequency table 3.2 Central parameters 3.3 Variation parameters 3.4 Graphical summary", " Chapter 3 Descriptive statistics Choose appropriate summary statistics (tables, graphics, parameters) to describe a population or a sample Interpret summary statistics 3.1 Frequency table One type of data statistical summary are frequency table, or contingency table. To summarize a qualitative variable, the frequency of the statistical units in each modality (category) of the variable is computed. The frequency can be reported as absolute count (absolute frequency) or proportion (relative frequency or count). For instance, in Table 3.1 the partition of students according to the school status is summarized (HBSC dataset). Table 3.1: Absolute frequency and relative frequency of pupils in each school type in the French HBSC database in 2006. Count Proportion (%) private 115 23 public 385 77 Total 500 100 In rows are the modalities (categories) of the variable “school status” in the original database and in columns are the frequency of students in each category. The first column is the absolute frequency or count and the second column is the relative frequency or proportion out of the total of students. The margin total is essential to display for quick assessment of potential mistake or missing values. To summarize a quantitative variable into a contingency table, the numerical values first need to be grouped into classes, generating in fact like a categorical variable. Next, the frequency of the statistical units in each group (class) of the new variable is counted. In Table 3.2, first the quantitative variable age was used to create age groups and, next, the partition of students according to age group was summarized. Table 3.2: Frequency table to summarize age distribution of children in the HBSC database, in France in 2006. Count Proportion (%) NA 1 0.2 [11-13[ 173 34.6 [13-15[ 178 35.6 [15-17[ 148 29.6 Total 500 100.0 One must read [11-13[ as 11 years-old students being included (counted) in that group while 13 years-old students being excluded. NA stands for Not Attributed or missing values. When summarizing two (or more) variables in two-way (or more) table using frequencies via a statistical software you might have to look for the term pivot table. Table 3.3 summarize in absolute frequencies the different age groups and the smoking status. For relative frequencies, you need to decide which way to count (Table 3.4 and 3.5). You need to ask yourself: Who are you interested in? What is your denominator? Table 3.3: Repartition of smokers and non smokers among age groups in the HSBC sample, in France in 2006 Age group Smoking Status No Yes Total NA 1 0 1 [11-13[ 166 6 172 [13-15[ 161 17 178 [15-17[ 104 44 148 Total 432 67 499 Table 3.4: Proportion of smokers and non smokers among age groups in the HSBC sample, in France in 2006 Age group Smoking Status No Yes Total NA 100.00 0.00 100 [11-13[ 96.51 3.49 100 [13-15[ 90.45 9.55 100 [15-17[ 70.27 29.73 100 Among the [15-17[ years old, 29.7% smoke. Table 3.5: Relative distribution of age groups among smokers and non smokers in the HSBC sample, in France in 2006 Age group Smoking Status No Yes NA 0.23 0.00 [11-13[ 38.43 8.96 [13-15[ 37.27 25.37 [15-17[ 24.07 65.67 Total 100.00 100.00 Among the smokers, 65% are aged [15-17[ years old. The proportion of smoker seems to increase with age. We will verify this later using inferential statistics (see Chapter 5) 3.2 Central parameters A central parameter, or location parameter, is the numerical value around which are distributed most of the values of a serie of data. 3.2.1 Mean The (arithmetic) mean is the most well known and commonly (but not always appropriately) used central parameter. The arithmetic mean is the sum of the values divided by the number of values in the data serie. Mathematically, in a population the equation is: \\(\\mu = ({\\sum^{{i=N}}_{{i=1}} X_{i})/N}\\) In a sample, the equation is: \\(m = ({\\sum^{{i=n}}_{{i=1}} x_{i})/n}\\) Note: Greek letters are used for population and Roman letters for sample. The mean is also sometimes symbolized like \\(\\bar{X}\\) for population or \\(\\bar{x}\\) for sample Figure 3.1 presents a histogram that summarize the distribution of weight of French student in the HBSC sample (for histogram definition see section 3.4 and sample definition see 4.1). From the graphic, the central point (pick) of the distribution, around which values of the data serie are spread is around the weight class [40-45[ Kg. Figure 3.1: Distrubution of weights (Kg) of 11 to 16 years-old students, in France in 2006 On average, in 2006 the French students aged 11 to 16 weighted 48 Kg (dashed red color). Why the mean is not between 40 and 45Kg? The advantages: Easy to understand Easy to compute The drawbacks: Sensitive to outlier: each value of the data serie count with the same weight Sensitive to the distribution shape 3.2.2 Median The mean is not always the appropriate statistical indicator to summarize the distribution data and should be sometimes replaced by the median. The median is the middle value of a ordered data serie. The median split the data serie in two part of equal number of data. Figure 3.2: Mean or Median, that is the question? In Figure 3.2, 50% of the statistical units, i.e 142 children hair sample, have mercury concentration below 1.8 \\(\\mu g/g\\) while 50%, i.e 142 children hair sample, have mercury concentration above 1.8 \\(\\mu g/g\\). If you had rely on the mean you would have said that on average children hair contain 4 \\(\\mu g/g\\) of mercury which wrongly make you believe that represent most of the children case. How to compute a median? Sort values in increasing order If there are an odd number of observations, find the middle value 2’. If there are an even number of observations, find the middle two values and average them Would you use the median or the mean to compare French region rainfall? The advantages: Easy to compute Not sensitive to outlier Less sensitive to skewed distribution than the mean Easy to understand The drawbacks: Sensitive to the distribution shape No idea of the minimum and maximum Easy to understand but need to be explicitly exposed 3.2.3 Percentile and quantile When looking at height distribution, the median is the exact middle value when people are ordered by height which correspond to the 50\\(^{th}\\) percentile or 50% below and above that value (Figure 3.3). But you could pick any percentile like the 80\\(^{th}\\) with 80% below and 20% above. The n\\(^{th}\\) percentile of a set of data is the value at which n percent of the data is below it. Figure 3.3: Percentile Percentiles can be calculated using the formula \\(n = (P/100)*N\\), where P = percentile, N = number of values in a data set (sorted from smallest to largest), and n = ordinal rank of a given value. A student scores in the 75\\(^{th}\\) percentile of his class. What does that mean? The 75\\(^{th}\\) percentile is also the 3rd quartile. The quartile split the sorted data values into quarters. The quartiles are the values that frame the middle 50% of the data (median or Q2). One quarter of the data lies below the lower quartile, Q1 (25% or 25\\(^{th}\\) percentile), and one quarter of the data lies above the upper quartile, Q3 (75% or 75\\(^{th}\\) percentile). Figure 3.4: Quartiles Using the R statistical software and the HBSC data set we can quickly describe the “Weight” variable of the French student aged 11 to 16 in 2006 with the five-number summary. The five-number summary provides a good overall look at the distribution of the data. summary(hbsc$Weight) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 24.50 38.83 47.00 48.09 56.00 90.00 18 There was 18 missing values (NA) The minimum weight was 24.5 Kg The maximum weight was 90 Kg The mean (average) is 48.09 Kg The median is 47 Kg meaning that 50% of the students weighted less than 47Kg and 50% of the students are heavier. The 1\\(^{st}\\) quartile is 38.8Kg meaning that 25% of the students weighted less than 38.8Kg and 75% weighted more. The 3\\(^{rd}\\) quartile is 56Kg meaning that 75% of the students weighted less than 56Kg and 25% weighted more. Several algorithms exit to compute quantile (for instance see ?quantile in the R staistical software). They rely on different definition the underlying distribution of the sample: discontinuous or continuous. In your case, no need to go into the details but you should know how to interpret the values. Mean versus Median If the data is normally distributed, i.e. bell shape, as statisticians like it (bottom of Figure 3.5), feel free to use the mean.he mean is easier to communicate and so if you can use it, use it. In fact the value of the mean should be really close the value of the median and the mode (or modal class). If your data is skewed (top of Figure 3.5), or there are large outliers, then use the median to find the centre of the data. Better yet, report both the mean and the median since any differences will reveal information about the presence of skew/outliers. Figure 3.5: Bell shape distribution A more subtle rule: if you are more concerned with the total sum, rather than the typical value, use the mean. For instance, if you have a salary cap and you are interested in the average salary of your players, use the mean. In this case, the mean is biased towards the high earners, and you really care about the high earners because they are the ones who are eating up your salary cap. 3.2.4 Mode The most frequent value or modality It is the only statistical parameter for the qualitative variables. Table 3.6: Frequency of physical activities (sport) in the HSBC sample, in France in 2006 Sports frequency Number NA 0 never 33 less than once a month 30 once a month 21 2 or 3 times a week 175 4 to 6 times a week 81 every day 63 In table 3.6 the mode is the category “2 or 3 times a week” with 175 students out of 500 practicing that much sports. For quantitative variables it could be a number or a class interval as in Figure 3.1 where the modal class is [40-45[Kg. Likert scale data. A Likert Scale is a type of rating scale used to measure attitudes or opinions. Five to seven items are usually used in the scale. In a survey with a 1-5 scale of “1-Very bad”, “2-Bad”, “3-Neutral”, “4-Good” and “5-Very Good” categories, the mean result across many participants came out to be 3.5. But what does 3.5 even mean in this context? Half way between Neutral and Good : Neutood? In terms of best practice, use the median when describing the centre of Likert data. Some may even argue for only using the mode on Likert data. 3.3 Variation parameters A variation parameter is numerical value which describes the dispersion of all the values of a serie of data around its location parameter 3.3.1 Range and IQR The range is the difference between the minimum and maximum value of a data serie. It is better to report the boundaries rather then the output of the difference because otherwise we do not know from where it starts. In which hospital will you go for emergency care? Figure 3.6: Hospitals ER The interquartile range (IQR) summarizes the spread by focusing on the middle half of the data. It is defined as the difference between the two quartiles: \\(IQR = Q3 - Q1\\). As the range, the IQR should be reported as an interval. For example, the five-number summary above on HBSC weight data show us that IQR is [38.8-56] Kg. 3.3.2 Variance and standard deviation A powerful measure of spread is the distance of the values of a data serie to its mean. The variance is the average squared differences to the mean. Based on the mean, the variance is appropriate only for symmetric data and can be influenced by outlying observations. Mathematically, if we tried to average the distances of all the values a data serie to its mean, the positive and negative differences would cancel each other out, giving an average deviation of 0-not very useful. Instead, we square each distance to get the variance. In a population the equation is: \\(\\sigma^2=\\frac{\\sum_{1}^{N}(X_i-\\bar{X})^2}{N}\\) where \\(\\sigma^2\\) is the variance of population \\(X_i\\) is the \\(i^{th}\\) value in the population \\(\\bar{X}\\) is the mean in the population \\(N\\) is the size of the population In a sample, the formula is: \\(s^2=\\frac{\\sum_{1}^{n}(x_i-\\bar{x})^2}{(n-1)}\\) where \\(s^2\\) is the variance of the sample \\(x_i\\) is the \\(i^{th}\\) value of the data serie \\(\\bar{x}\\) is the mean of the data serie \\(n\\) is number of values in the data serie The variance plays an important role in statistics, but as a measure of spread, it has a problem. Whatever the units of the original data, the variance is in squared units. To express the spread to in the same units as the data we take the square root of the variance. That gives the standard deviation. A standard deviation is the square root of the variance, the average differences to the mean. Mathematically, in a sample the formula is: \\(s=\\sqrt\\frac{\\sum_{1}^{n}(x_i-\\bar{x})^2}{(n-1)}\\) In the HBSC sample, one average pupils weight 48.09 Kg +/- 12.25Kg. It means that the average variation of the weights around the mean is of 12.25Kg. However some pupils can be lighter than 35.75kg (48-12.25 - the minimum is in fact 24.5Kg) and some pupils can be heavier than 60.25kg (48+12.25 the minimum is in fact 90Kg). 3.4 Graphical summary Graphical representations, as summary in tables, must be self-content and self-explanatory. They should be readable without text around. Plot title should include the W’s Who are you representing, What characteristics, Where and When it is happening. Note: I will be intransigent on that matter 3.4.1 Barplot For one qualitative variable, the frequency barplot, or barchart, is the most appropriate plot. It could use the absolute or relative frequencies. Figure 3.7: Repartition of students according to their sport activity level, in France in 2006 (source:HBSC) Figure 3.7 represents the distribution of students (Who) according to their sport activity level (What), in France (Where) in 2006 (When) and if available the source should also be cited (here HSBC) in the caption. It is a better practice to multiply plots that combining too many information in one plot. Imagine you would like to display the same information but for boys and girls. The below barplot is better than the next one as you can easily compare boys and girl but you also easily visualize the trend within each group. Figure 3.8: Repartition of students according to their sport activity level, in France in 2006 (source:HBSC) Figure 3.9: Repartition of students according to their sport activity level, in France in 2006 (source:HBSC) 3.4.2 Pie chart For one qualitative variable, you can also use a pie chart. However you should be careful on the number of modalities the variable to display can have: too few, the plot take lots of room for few information but too many you will rapidly get “the wheel of fortune”. Figure 3.10: Repartition of students according to their sport activity level, in France in 2006 (source:HBSC) My advice would be to avoid using that plot but if you absolutely need to use it do not forget to display number and/or % in the sectors of the plot so reader do not have to do the math. 3.4.3 Histogram For one quantitative variable, the histogram is often used to visualize the shape of the distibution of the data serie. Although one have to be careful of the bin width (breaks) use to group the numerical values. Figure 3.11: Distribution of students weight in France in 2006 (source: HBSC), binwidth=1 Figure 3.12: Distribution of students weight in France in 2006 (source: HBSC), binwidth=10 Figures 3.11 and 3.12 present the same data serie but with a bin width of 1 and then 10. The shape of the plots are different, the modal class varies. With a bin width of 1 you have no summary and too many details while a bin width of 10 may be compact. There is no recipe, statistical software helps you with default algorithm but my advice would be to try a couple of bin widths and select the one you believe summarize the best the data Barplot versus Histogram Barplot are for qualitative data. The bars are separated. Histogram are for quantitative data. The bins are joined. Gaps may occurred when no one fall in a particular bin. If you want to compare the distribution of one quantitative variable between more than 2 groups (qualitative) the histogram remain interesting, as shown below. But up to three the comparison start to be difficult. A boxplot is then more appropriate. Figure 3.13: Distribution of students weight by gender group in France in 2006 (source: HBSC) 3.4.4 Boxplot A boxplot helps presenting the five-number summary classically used to describe a sample. The central box shows the middle half of the data, between the quartiles. The top of the box is at the third quartile (Q3) and the bottom is at Q1, the height of the box is equal to which is the IQR. The median is displayed as a horizontal line. If the median is roughly centered between the quartiles, then the middle half of the data is roughly symmetric. If it is not centered, the distribution is skewed. In extreme cases, the median can coincide with one of the quartiles. Figure 3.14: Boxplot elements The whiskers reach out from the box to the most extreme values that are not considered outliers according to John W. Tukey’s rule. The boxplot nominates points as outliers if they fall farther than 1.5*IQRs beyond either quartile. They may be mistakes or they may be the most interesting cases in your data. This rule is not a definition of what makes a point an outlier. It just nominates cases for special attention. Figure 3.15: Distribution of students weight by group of sport activity level, in France in 2006 (source: HBSC) In Figure 3.15 the distribution of students’ weight vary by group of sport activity level. We note 3 extrema: 2 in the “2 or 3 times a week” group and 1 in the “once a month” group. Overall the boxplots overlap suggesting than on average the weights between groups might not be statistically different (see Chapter 5). Let’s interpret the “never” group (bottom boxplot): around 50% of the students in that group weight 45Kg or less, 75% of the students weight 55Kg or less while 25% weight 55Kg or more What proportion of students lies between 42 and 55 Kg? 3.4.5 Scatterplot For two quantitative variables, a scatterplot is used to assess if there is a relationship between the two variables. Figure 3.16: School results and BMI among students in France in 2006 (source: HBSC) Figure 3.16 displays the school results and BMI among students in France in 2006. It appears that there is no linear relationship between the two variables (see Chapter 5 and 6.1). 3.4.6 Communication tips Avoid 3D plots our eyes are not good at visualizing 3D and mathematically it’s often wrong you do not manipulate volumes but numbers. Figure 3.17: Bad visual 1 Figure 3.18: Bad visual 2 Use appropriate colors Proscribe rainbow plots Sequential scale (gradiant) are suited to ordered data Diverging palettes put equal emphasis on mid-range critical values and extremes at both ends of the data range -Qualitative palettes do not imply magnitude differences between legend classes, and hues are used to create the primary visual differences between classes. Do not forget our color-blind friends ! What summary statistics should you use: tables, graphics or statistical parameters? It all depends. Depend on the objective Return the data “take home message” Depend on the audience Expert Everybody else Depend on the data type Qualitative Quantitative Figure 3.19: Communication tips "],
["inferencestat.html", "Chapter 4 Inference and sample 4.1 Sample 4.2 Confidence intervals", " Chapter 4 Inference and sample Make the difference between individual variation and sample variation Make the difference between observed values and estimated parameters Interpret a confidence interval Interpret a statistical test 4.1 Sample 4.1.1 Population versus Sample Why take a sample … to conclude on the population? Figure 4.1: Population versus Sample When the population is small and isolated, like an Inuit village in Greenland for instance, it is relevant to study/interrogate the entire population to get the exact prevalence of a rare disease (Figure 4.1). However if the size of the population is large we might not be able (want) to get it all! It is time consuming, costs a lot, and we are smarter. Although we will conclude on the population, a well design sample allows an estimation of the prevalence of the outcome of interest. What is the main property of a good sample? Since we draw conclusions about a population based on the information obtained from a sample (subset of the population), it is important that the units of interest in the sample are representative of the entire population. A representative sample will allow to confidently generalize the results and the conclusion of your study Poor sampling designs can yield misleading conclusions Set of observations drawn from a population All the individuals of the study population should have the same probability of being drawn A representative sample should be an unbiased reflection of what the targeted population is like for the units of interest (outcome and covariates/determinants) A representative sample for a variable should reflect the variable distribution observed in the targeted population. For example, you plan to determine the relationship between gratitude and job satisfaction in gynecologists. Your sample might consist of 30 to 40 gynecologists. Your population might be “gynecologists in the United States”, or, if the scope of your study was more narrow, “gynecologists in New York City”. So, if most gynecologists in the population are women, but your sample is all male, you do not have a good case for representativeness because your sample does not share the same characteristics as the larger population. In this case, you cannot generalize the results of your study to the population. 4.1.2 Sample designs Element selection technique Probability sampling Non-probability sampling Unrestricted sampling Simple random sampling Convenience sampling (Voluntary response) Restricted sampling Complex random sampling (systematic sampling, stratified sampling) Purposive sample (such as quota sampling) 4.1.3 Probability sampling Simple random sample of size n consists of n individuals from the population chosen in such a way that every set of n individuals has an equal chance to be the sample actually selected. The simple random sampling is the gold standard: All individuals are given an equal chance to be chosen to be in the sample. Figure 4.2: Simple random sample (source: Academic Web Services) Stratified random samples (or block experiment) define by strata of similar characteristics and choose a separate simple random sample (SRS) in each stratum and combine these to form the full sample. Figure 4.3: Stratified sampling (source: Academic Web Services) Cluster and multistage samples consist of all or random units within clusters (hierarchical sampling) Figure 4.4: Cluster sample (source: Academic Web Services) For example, the HBSC study is based on a multistage sampling. In France, in each region some schools were randomly selected, within a school some classes were randomly selected and all the students of the selected class were interrogate. The Dean of the School of public health wants to meet with several members of the faculty of the Department of Statistics to discuss concerns they might have about their department. He does not have time to talk to all of the faculty members, but wants to be sure that he meets with faculty with various levels of seniority. The Department of Statistics is composed of: 18 Professors 3 Associate Professors 5 Assistant Professors The Dean decides to speak with 5 Professors, 2 Associate Professors, and 3 Assistant Professors. What sampling design can be used? Systematic sampling Sometimes we draw a sample by selecting individuals systematically. For example, a systematic sample might select every tenth person entering a school cafeteria (price, food quality, opening hours…). To make sure our sample is random, we still must start the systematic selection with a randomly selected individual, not necessarily the first person entering the cafeteria at lunchtime. When there is no reason to believe that the order of people entering the pool could be associated in any way with the responses measured. Figure 4.5: Systematic sampling (source: Academic Web Services) 4.1.4 Non-probability sampling Voluntary response survey In a voluntary response sample, a large group of individuals is invited to respond, and all who do respond are counted. This method is used by call-in shows, Internet polls, Political party survey. Voluntary response samples are almost always biased, and so conclusions drawn from them are almost always wrong. For example, the ABC news program Nightline once asked their viewers whether the United Nations should continue to have its headquarters in the United States. In order to have their opinions counted, viewers had to call a 1-900 number and pay a small fee. More than 186,000 callers responded and 67\\(\\%\\) said “No.” The callers of such program tend to be strong headed and often negative opinions. A nationwide poll with a proper sampling design found that less than 28 \\(\\%\\) of US adults want the UN to move out of the United States. Convenience sampling chooses the individuals who are convenient, the easiest to reach to be in the sample. The group is probably not representative of the targeted population. This method is used in shopping area (street, mall). This population tend to be more affluent and include a larger percentage of teenagers and retirees than the population at large. Figure 4.6: Convenience sample (source: Academic Web Services) For example, you interview people at the mall or on a street corner about the French school policy that allows every cities to decide whether school schedule is spread over 4 or 5 days per week. Other non-probability sampling methods exists but are largely bias. One has to be aware and truthful when concluding on such samples. Figure 4.7: Non-probability sampling methods 4.1.5 Sampling bias When a sample is biased, the summary characteristics of a sample differ from the corresponding characteristics of the population it is trying to represent. Undercoverage: some portion of the population is not sampled at all or has a smaller representation in the sample than it has in the population. Non-response: those who don???t respond may differ from those who do Comparability Wording of questions: influence the answers by presenting one side of an issue consciously or not Response bias: tendency to please the interviewer 4.2 Confidence intervals 4.2.1 Within and between sample variation Figure 4.8: Parameter estimation from sample to population Figure 4.9: Variation of individuals within a sample As shown in Figure 4.9, individual measures vary with a sample. The standard deviation \\(s\\) is a indicator of that individual fluctuation. It is the average distance of the measures of the individuals to the mean. Figure 4.10: Variation between a samples drawn from the same population If you drawn different samples from the same population you will get different means (Figure 4.10) due to sampling fluctuation or sampling distribution of the mean. This variance between the means of different samples can be estimated by the standard deviation of this sampling distribution and it is the standard error (SE) of the estimate of the mean. The standard error (SE) is a type of standard deviation for the distribution of the means of samples drawn from the same population. Standard Error: \\(\\sigma_\\mu = s/\\sqrt{n}\\) where: \\(\\sigma_\\mu\\) standard error of the mean \\(s\\) standard deviation of the sample (3.3.2) \\(n\\) size of the sample 4.2.2 The CLT and the confidence interval The mean \\(m\\) in a sample of size \\(n\\) is a random variable, which varies between samples. This random variables should follow a Normal distribution centered around \\(\\mu\\), the true mean of the population. This is the central Limit Theorem. Figure 4.11: The observed mean is a random variable Central Limit Theorem (CLT) The mean of a random sample has a sampling distribution whose shape can be approximated by a Normal model. The larger the sample, the better the approximation will be. For example, in the HBSC survey the observed mean of weights of 11 years-old French children in 2006 in our sample (n=500) is 38, 55 Kg (s = 8,10 Kg). How do we estimate the TRUE value in the population of children aged 11 in 2006 in France? Using central limit theorem, one can demonstrate that we can be 95% confident that the true mean \\(\\mu\\) of the population from which is extracted our sample is within the interval: \\(m-1.96*\\sigma_\\mu \\leq \\mu \\leq m+1.96*\\sigma_\\mu\\) where: \\(\\sigma_\\mu\\) standard error of the mean \\(m\\) mean of the sample 1.96 multiplier coefficient that depends on the confidence level expected Following the example, the CI95% will be [37.40Kg-39.85Kg]. Similarly the proportion \\(p\\) in a sample of size \\(n\\) is a ramdon variable, which varies between samples. This random variables follows a Normal distribution centered around \\(\\pi\\). For example, you are interested in the proportion of children (aged 2-3) with sleeping disorder in Isere, France (population: 14 000 children of 2 or 3 years old). You built a sample of 540 children among which 86 had sleeping disorder. The proportion of children with sleeping disorder in our sample is \\(p=16\\)%. But what is the TRUE proportion of children (aged 2-3) with sleeping disorder in Isere? Using central limit theorem, one can demonstrate that we can be 95% confident that the true proportion \\(\\pi\\) of the population from which is extracted our sample is within the interval: \\(p-1.96*\\sigma_\\pi \\leq \\pi \\leq p+1.96*\\sigma_\\pi\\) where: \\(p=k/n\\), \\(k\\) is the number of entities with the characteristic and \\(n\\) the size of the sample \\(\\sigma_\\pi\\) standard error of a proportion \\(\\sqrt{p(1-p)/n}\\) 1.96 multiplier coefficient that depends on the confidence level expected Following the example, the CI95% will be [12.9% ; 19.1%]. 4.2.3 Interpretation of confidence intervals The true mean is or is not in your estimated confidence interval. At 95% confidence, You took an 5%-risk of having the wrong confidence interval. Figure 4.12: Confidence interval estimation from 20 samples drawn from the same population. In Figure 4.12, 19 out of 20 samples (95%) from the same population will produce confidence intervals that contain the TRUE population parameter. There is a 95% probability that the calculated confidence interval encompasses the true value of the population parameter For example, in the HBSC sample and the age group [11-13[ years old, the mean weight is 38.61Kg. But what is the true mean in the population? ## n mean sd se lower95ci upper95ci ## 167 38.60539 8.156085 0.6311368 37.3593 39.85148 where: \\(n\\) is the size of the sample \\(sd\\) standard deviation of the sample \\(se\\) standard error of population mean \\(lower95ci\\) lower bound of the 95% confidence interval \\(upper95ci\\) upper bound of the 95% confidence interval We cannot give a unique true answer but only an interval (an estimation) with a certain confidence. We will say that in our sample the punctual estimate of the mean of weights for student aged 11 in France in 2006 was 38.60 kg and that we are 95% confident that in 2006, among the entire population of 11 year-old French student, the mean of weights is between 37.40Kg and 39.85Kg (CI95% [37.40Kg-39.85Kg]). For the proportion example, we are 95% confident that the true proportion of children, aged 2-3, with sleeping disorder in Isere is between 12.9% and 19.1%. 4.2.4 Why 1.96? We can generalize the equation above to: \\(m-Z_\\alpha\\sigma_\\mu \\leq \\mu \\leq m+Z_\\alpha\\sigma_\\mu\\) where: \\(Z_\\alpha\\) is the critical value of the Normal distribution (centered on \\(\\mu=0\\), reduced to \\(\\sigma=1\\)) where \\(100-\\alpha\\) % of the values stand within \\(Z\\) standard deviations of the mean. Figure 4.13: Centered reduced Normal distribution. The critical value \\(Z_\\alpha\\) will be larger or smaller when computing a confidence interval at 90? 4.2.5 Precision or Margin error The term \\(Z_\\alpha\\sigma_\\mu\\) or \\(Z_\\alpha\\sigma_\\pi\\) is noted \\(i\\) and named the precision or margin error of the confidence interval. In a simple random sample, the precision is proportional to the square root of the inverse of the sample size. It is not linear! Figure 4.14: Precision to sample size relationship in a simple random sample. For example, what should be the size of the sample to estimate the prevalence of overweight children aged 12, in Haute Savoie with a precision of \\(i\\) knowing from the literature that the estimate prevalence is 17%? From the equation of the confidence interval of a proportion is \\(n=P(P-1)Z_\\alpha^2/i^2\\) For a precision of 3%, \\(n = 0.17(1-0.17)*1.96^2/0.03^2 = 627\\) For a precision of 1%, \\(n = 0.17(1-0.17)*1.96^2/0.01^2 = 5644\\) !!! "],
["tests.html", "Chapter 5 Inference and statistical tests 5.1 Formulate a hypothesis 5.2 Comparison of two means 5.3 Comparison of two proportions 5.4 Risk \\(\\alpha\\) and \\(p-value\\) 5.5 Risk \\(\\alpha\\) and risk \\(\\beta\\) 5.6 Comparison of multiple groups 5.7 Parametric and non-parametric test", " Chapter 5 Inference and statistical tests Understand statistical test reasoning Interpret results of a statistical test Discuss significance of a statistical test The aim of a statistical test is to reach a scientific decision on a difference (or effect), on a probabilistic basis, based on observed data. When assessing differences between groups (Who), you have to define What to compare. According to the type of the variable, you will choose a statistical parameter (mean, proportion, data distribution) to perform the comparison. The comparison will be based on hypothesis, with possible assumption to verify, and the associated statistical test. In summary, the procedure is as follow: Formulate hypothesis to be tested Choose the appropriate statistical test Calculate the appropriate statistic measure Interpret the result 5.1 Formulate a hypothesis In hypothesis formulation you always have two possibilities: it is not different OR it is different. In the HBSC data, we are interested in the characteristics of the smoking students compare to the non-smoking. Do they differ by some characteristics? Example 1: we would like to test whether there is, on average, a difference in height between smokers and non-smokers. Example 2: we would like to test if the proportion of smokers varies according to gender. First, we describe the distribution of the variable between the groups. In example 1, the height is a quantitative continuous variable which can be summarized by the mean (Table 5.1). In our sample, the students who do not smoke measure on average 157 cm while the students who smoke measure in average 166 cm. The question is “At the population level, is that different knowing that you do have individual variation (SD) and sample variations (SE)?”. Table 5.1: Description of the Height (cm) variable by smoking group (0=non-smoking, 1=smoking) in the French HBSC database in 2006. SmokingStatus Mean SD Median Q1 Q3 0 157.89 12.00 159 149 166.25 1 166.26 11.39 168 159 173.00 In example 2, the gender is a qualitative variable which can be summarized into proportions (Table 5.2). In our sample, the proportion of students seem to different between groups. The question is “At the population level, are those proportions real different knowing that you do have individuals’ variation (sd) and sampling variation (se)?”. Table 5.2: Proportion of students smoking (1) or non-smoking (0) by gender in the French HBSC database in 2006. Smoking status Gender No Yes boy 85.26 14.74 girl 87.90 12.10 In theory, we test whether the two groups (samples) come from the same population (Figure 5.1). For instance, sample 1 with mean m1 from population 1 with \\(\\mu_1\\) is coming from the same population as sample 2 with mean m2 from population 1 with \\(\\mu_2\\). Population 1 is equal to population 2. Figure 5.1: Population versus Sample When you state the hypothesis you should explicit H0 an H1. H0 : The null hypothesis is that the parameters are EQUAL, i.e they are not different. H1: The alternative hypothesis is that the parameters are NOT EQUAL, i.e they are different. I like to write down the hypothesis with the \\(=\\) sign and \\(\\neq\\) as I find easier to pick the test and interpret the results afterward. Remember the W’s they should appear in the hypothesis if you have the information. Example 1: we would like to test whether there is, on average, a difference in height between smokers and non-smokers. H0: In France in 2006, the mean height of the students 11-16 who smoke was equal to the mean height of the students 11-16 who do not smoke. H1: In France in 2006, the mean height of the students 11-16 who smoke was NOT equal to the mean height of the students 11-16 who do not smoke. Example 2: we would like to test if the proportion of smokers varies according to the groups of ages. H0: In France in 2006, the proportion of student girls 11-16 who smoke was equal the proportion of student girls 11-16 who do not smoke and the proportion of student boys 11-16 who smoke was equal the proportion of student boys 11-16 who do not smoke. H1: In France in 2006, the proportion of student girls 11-16 who smoke was NOT equal the proportion of student girls 11-16 who do not smoke and the proportion of student boys 11-16 who smoke was NOT equal the proportion of student boys 11-16 who do not smoke. A statistical test is always performed to answer the H0 hypothesis (Figure 5.2). When we prove that the statistical parameters differ we reject H0 and accept H1. We say that we observed a statistically significant difference between the parameters. When we cannot prove that the statistical parameters differ, we stay under H0 and say that: we fail to reject H0 because we cannot show any statistically significant difference. H0 is never accepted as an error risk still exists that we can not compute. Figure 5.2: Statistical test interpretation 5.2 Comparison of two means Once the hypothesis are stated, we choose a test. In the context if the comparison of means, the test will assess whether the observed difference (\\(\\Delta\\)) between the two groups is random (due to individuals’ and sampling variations) or not (Figure 5.3). In theory, if H0 is true \\(\\Delta = m_1 - m_2 = 0\\) Figure 5.3: Population versus Sample But we never compare \\(\\Delta\\) to 0 as we need to take into account the individuals’ fluctuation (sd) and sampling (se) variation. What is the critical value, then ? Can you guess ? The critical value depends on the risk you are willing to take to conclude about a difference that does not exist in reality, the \\(\\alpha\\) risk. When comparing means, you make the assumption that your sample’s distributions are not too different from a Normal distribution. Therefore, the difference \\(\\Delta=m_1-m_2\\) should follow a Normal distribution centered on 0. Then to take into account the individuals and sampling variation, the difference is standardized. The statistical value \\((m_1-m_2)/s_\\Delta\\) is computed and compare to the critical value \\(Z_\\alpha\\) of the centered reduced Normal distribution for a risk \\(\\alpha\\). [Note \\(s_\\Delta\\) is function of the chosen test] For a risk \\(\\alpha=5\\)% the critical value is \\(Z_\\alpha=1.96\\). If H0 is true, 95% of values of \\((m_1-m_2)/s_\\Delta\\) are between -1.96 and 1.96. If the statistical value \\((m_1-m_2)/s_\\Delta\\) returned by your test is above 1.96 or below -1.96, you reject H0 and accept H1. In example 1, the statistical value for a risk \\(\\alpha=0.05\\) is -5.4582. What is you conclusion? 5.3 Comparison of two proportions In the context of proportion comparison like in example 2, the objective is to assess whether the proportion of cases among the exposed group is equal to the proportion of cases among the non-exposed group. Example 2: we would like to test if the proportion of smokers varies according to the groups of ages. H0: In France in 2006, the proportion of student girls 11-16 who smoke was equal the proportion of student girls 11-16 who do not smoke and the proportion of student boys 11-16 who smoke was equal the proportion of student boys 11-16 who do not smoke. H1: In France in 2006, the proportion of student girls 11-16 who smoke was NOT equal the proportion of student girls 11-16 who do not smoke and the proportion of student boys 11-16 who smoke was NOT equal the proportion of student boys 11-16 who do not smoke. To this aim, as for the comparison of means, we will compute the standardized differences (distances) between groups. 5.3.1 Chi-square test The Chi-square is often the test used as rather intuitive and non computer greedy. First, we compute a two-way table with your observed counts: Table 5.3: Observed number of boys and girls students exposed or not to smoking in the French HBSC database in 2006. Smoking status Gender No Yes Total boy 214 37 251 girl 218 30 248 Total 67 432 499 Under H0 the absence of difference (independence assumption), we would expect to have the same proportion of patients among those exposed and those not exposed. So keeping the total margins what would be the expected counts? (Table 5.4) Table 5.4: Expected number of boys and girls students exposed or not to smoking in the French HBSC database in 2006. Smoking status No Yes Total boy ? ? 251 girl ? ? 248 Total 432 67 499 To compute a theoretical two-way table with your expected counts: the proportion exposed student is: \\(p= 67/499= 0.134\\), i.e 13.4%. the number of boys students exposed would be: \\(p= (67/499)*251= 33.7\\). Table 5.5: Expected number of boys and girls students exposed or not to smoking in the French HBSC database in 2006. Smoking status No Yes Total boy ? (67/499)*251 251 girl ? ? 248 Total 432 67 499 Table 5.6: Expected number of boys and girls students exposed or not to smoking in the French HBSC database in 2006. Smoking status No Yes Total boy 217.3 33.7 251 girl 214.7 33.3 248 Total 432 67 499 Then we compute the Chi-square (\\(\\chi^2\\)) statistic which is the standardized sum of the differences between the observed and the expected value. \\(\\chi^2_{Obs}= \\sum(\\dfrac{(Obs-Exp)^2}{Exp}\\) Using the R statistical software, we have chisq.test(hbsc$Gender, hbsc$SmokingStatus) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: hbsc$Gender and hbsc$SmokingStatus ## X-squared = 0.54013, df = 1, p-value = 0.4624 The \\(\\chi^2_{Obs} = 0.54\\). Now the question is: “is it equal to 0?” As for the comparison of means, in theory, if H0 is true \\(\\chi^2_{Obs} \\sim 0\\) but it is never 0. There are variations and we test H0 with a \\(\\alpha\\) risk. Therefore, what is the threshold? To define that threshold, we need to choose the correct statistical law. The \\(\\chi^2\\) distribution depends on k, the number of degrees of freedom (df) which depends on the number of characteristics of the two variables we are comparing (Figure 5.4). Figure 5.4: Chi2 distribution depends on the degree of fredom K When the two-way table of the expected number is drawn, the degree of freedom is the number of values in the final calculation that are free to vary. For instance, in a 2x2 table once you have set one value the others cannot change. The quick formula to compute the degree of freedom (df) for the \\(\\chi^2\\) distribution is the number of rows in the table minus 1 multiply by the number of columns in the table minus 1: df = (#rows - 1) * (#cols - 1) For a 2x3 table, what is the degree of freedom? Next, we need to look at the statistical table of the \\(\\chi^2\\) law (Figure 5.5). The threshold value also depends on the \\(\\alpha\\) risk you are willing to take. Figure 5.5: Chi2 distribution depends on the degree of fredom K In the \\(\\chi^2\\) table, for our 2x2 table the \\(df = 1\\) and for a \\(\\alpha\\) risk of 5%, the \\(\\chi^2_{Theo}= 3.84\\). Next the decision rule is the same as for the comparison of means. In example 2, the statistical value for a risk \\(\\alpha=0.05\\) and \\(df=1\\) is 3.84. The \\(\\chi^2_{Obs} = 0.54\\). What is you conclusion? To correctly use the \\(\\chi^2\\) test and have accurate estimation of the associated probabilities, we need to have at least n=5 count in each cell of the table of expected numbers. 5.3.2 Fisher’s Exact test To compare proportions the Fisher’s Exact test is even better than \\(\\chi^2\\) as it computes the exact probability of obtaining a difference even greater if H0 is true. However the formula is more complex and difficult to compute by hand . A computer is highly recommended (Table 5.7). Table 5.7: Fisher Exact test principal. Smoking status No Yes Total boy a c n1 girl b d n2 Total t1 t2 N \\(p = \\dfrac{n_1!n_2!t_1!t_2!}{a!b!c!d!}\\) Using the R statistical software, we have fisher.test(hbsc$Gender, hbsc$SmokingStatus) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: hbsc$Gender and hbsc$SmokingStatus ## p-value = 0.4316 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.4571282 1.3779096 ## sample estimates: ## odds ratio ## 0.7962999 How to read the output of the Fisher’s Exact test? We can look at the odds ratio and the confidence interval (95% CI) As it is a ratio (numerator/denominator) if there is no difference (numerator=denominator) the \\(odds ratio \\sim 1\\). However it is never 1, we need to look at the confidence interval (at a certain risk level) to conclude. If the confidence interval includes 1, we fail to reject H0, we can not conclude that the proportions differ. If the confidence interval does not include 1, we reject H0, the proportions differ. In example 2, the Fisher’s exact test returns an odds ratio of 0.79 and 95% CI [0.45, 1.37]. What is your conclusion? We can look at the \\(p-value\\) but what is a \\(p-value\\) ? See next section 5.4. 5.4 Risk \\(\\alpha\\) and \\(p-value\\) The demonstrations above and the use of \\(Z_\\alpha\\) or \\(\\chi^2_\\alpha\\) values are valid for comparison of means or proportions under some assumptions and conditions (\\(Z_\\alpha\\) with sample size above 30 in each group; \\(\\chi^2_\\alpha\\) function df…). What is happening for other tests? The philosophy is exactly the same but the critical value might come from other statistical laws than the Normal law (Chi-square, Binomial, Poisson…). It might be difficult to retrieve the critical value need to compare to your computed statistical value. There are more statistical tables that there are statistical tests. The common practice is then to compare the risk \\(\\alpha\\), defined a priori, to the \\(p-value\\) returned by the test a posteriori.We want to know the ultimate risk that is taken. Meaning, the risk corresponding to the value found by the test. \\(\\alpha\\) risk and \\(p-value\\) \\(\\alpha\\) risk: a priori risk to conclude about a difference that does not exist in reality \\(p-value\\): a posteriori error risk that is taken knowing the result of the test Statistical test’s decision rule When \\(p-value\\) &gt; \\(\\alpha\\) risk, we FAIL to reject H0 When \\(p-value \\leq \\alpha\\) risk, we reject H0 The \\(p-value\\) is not synonymous of the importance of the possible difference between groups. In other words, a very small p-value means that the risk of making a mistake is very low. It does not mean that there is a huge difference between groups. Example 3: Using 2 studies we are assessing 2 new methods (A and B) for the prevention of surgical site infections (SSI) compared to a conventional method (0). In study A: method A shows 12% of SSI and the conventional method 24% of SSI. The test between A and 0 returns a \\(p-value \\leq 0.05\\). In study B: method B shows 12% of SSI and the conventional method 24% of SSI. The test between B and 0 returns a \\(p-value \\leq 0.001\\) Is method B better than method A in preventing SSI? In the example above, we cannot tell if method B is better than method A : we did not test A versus B. The only information we get is that method A is different from method 0 and that method B is different from method 0. In fact method A and B present the same level of SSI. They might not be different but to conclude (with a certain level of confidence) we need to do a test. (see section 5.7 for comparison of multiple groups) In example 1, we tested H0 with the Student’s T test. The \\(p-value\\) is 4.342e-07. What is your conclusion? In example 2, we tested H0 with a \\(\\chi^2\\) test. The \\(p-value\\) is 0.462. What is your conclusion? We also tested H0 with the Fisher’s Exact test. The \\(p-value\\) is 0.431. Do you reach the same conclusion than with the \\(\\chi^2\\) test? Why? 5.5 Risk \\(\\alpha\\) and risk \\(\\beta\\) Why do we not accept H0? As mentioned earlier there is always a risk of being wrong (Figure 5.6) but that risk cannot be computed. It is \\(\\beta\\). In row the unknown truth, in column the conclusion of the test. Figure 5.6: Where do the risks stand? \\(\\alpha\\) is the probability of rejecting H0, when H0 is true (Figure 5.7) \\(\\beta\\) is the probability of failing to reject H0, when H1 is true (Figure 5.8) Figure 5.7: Population versus when it does not exist Imagine that you are the “eyes” and you are unable to see the perspective above the horizon. You will believe that the 2 lines make 1 and that they are of the same length but in reality the further away is longer. That is the \\(\\beta\\) risk. Figure 5.8: Not seeing a difference when it does exist. What do you prefer \\(\\alpha\\) or \\(\\beta\\) ? It is a difficult question. Example 4: Hepatitis B vaccination and multiple sclerosis. Many study protocols have been conduct to assess an possible association between vaccination against Hepatitis B and the development of multiple sclerosis. The null hypothesis was that prevalence of multiple sclerosis was the same among people vaccinated against Hepatitis B and people not vaccinated against Hepatitis B. In that context, what would you favour \\(\\alpha\\) or \\(\\beta\\)? Wrongly reject H0 and conclude that there is an effect : it could be catastrophic as the immunization coverage will fall down Not seeing an effect: people may not get Hepatitis B vaccination but multiple sclerosis !!! Note that there have been many studies on the above question and that no effect as been seen so far. You test the hypothesis of an absence of difference in school grades between gender. What is your conclusion if: \\(p-value = 0.049\\) \\(p-value = 0.051\\) Comment on your conclusions. 5.6 Comparison of multiple groups Example 5: We are interested in the effect of 2 treatments to gain weight. The protocol include treatment A, treatment B, and a placebo group. We would like to compare the weights between the 3 groups. How do we compare the means? Can we do 2 by 2 comparisons? Table 1: Effects of 2 treatments to gain weight Parameters Treatment A Treatment B Placebo sample size n 62 62 96 mean weight 66.06 63.83 62.32 sd weight 3.18 2.16 3.12 To the aboce question the answer is “No” : it increases the likelihood of incorrectly concluding that there are statistically significant differences, since each comparison adds to the probability of a type I error, \\(\\alpha\\). At the end, if \\(k\\) is the number of comparisons, the error rate becomes \\(1-(0.95)^k\\) 5.6.1 Graphical comparison A boxplot and whisker plot (section 3.4) is an ideal graphical representation to compare data series of the same variable between different groups. Figure 5.9 shows that the distributions seems to differ. The median (black line with the box) are located at differ weight. Figure 5.9: Boxplot and whisker plot of the effect of different treatments on gain weight What are the IQR of the 3 groups? In Figure 5.9 the medians are not in the middle of the box. This suggest that the distributions might be skewed. Figure 5.10 presents density plots, similar to histograms, and reveals the same thing. Figure 5.10: Density plots of the effect of different treatments on gain weight The next step is thus to statistically test the hypothesis of equality of means. 5.6.2 Analysis Of Variance The Analysis Of Variance or ANOVA allows comparing multiple groups The Analysis Of Variance or ANOVA systematically compare variability within and between groups. When the variations observed between groups is greater than the within group variation, at least one group is differ from the other. The statistical hypotheses are: - H0: \\(\\mu1 = \\mu2 = \\mu3 ... = \\mu k\\) with \\(\\alpha\\)=5% - H1: At least one mean is different from the other where \\(k\\) is the number of independent groups To that aim we use the F-test (named in honor of Sir Ronald Fisher). The F-statistic is a ratio of two variances that examine variability. \\(F = \\frac{Mean Square Between}{Mean Square Error}= \\frac{MSB}{MSE}\\) between groups being compared (Mean Square Between or Mean Square Treatment) within the groups being compared (Mean Square Error or Mean Square Residuals) Mean Squares Sums of Squares (SS) DF MSB = SSB/(k-1) \\(SSB = \\sum_{i=1}^k n_i(\\bar{X_i}-\\bar{X})^2\\) k-1 MSE = SSE/(N-k) \\(SSE = \\sum_{i=1}^k\\sum_{j=1}^n(X_{ij}-\\bar{X_i})^2\\) N-k total \\(SST = \\sum_{i=1}^k\\sum_{j=1}^n(X_{ij}-\\bar{X})^2\\) N-1 where DF = degree of freedom \\(X_{ij}\\) = individual observation j in treatment i \\(\\bar{X_i}\\) = sample mean of the \\(i^{th}\\) treatment (or group/sample) \\(\\bar{X}\\) = overall sample mean k = number of treatments or independent groups n = number of observations in treatment i N = total number of observations or total sample size In practice with R res &lt;- aov(Weight ~ Treatment, data=treatFrame) summary(res) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 2 270.5 135.26 16.48 2.62e-07 *** ## Residuals 183 1501.7 8.21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F-value is the F statistics. The F-value = 16.48 (=135.26/8.21). To conclude under H0 and a given \\(\\alpha\\) risk, we can try to look for the appropriate statistical law and its associated table or we can conclude using the p-value. The \\(p-value = 2.62e^{-07}\\). What is your conclusion? 5.6.3 Post-hoc analysis and ANOVA assumptions The ANOVA results might help you conclude that a least one group varies differently that the others. However you will not know which group. To that aim you, need to perform a post-hoc analysis using the TukeyHSD’s test. In practice with R ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Weight ~ Treatment, data = treatFrame) ## ## $Treatment ## diff lwr upr p adj ## B-A -1.667323 -2.883055 -0.45159037 0.0040296 ## P-A -2.945509 -4.161241 -1.72977619 0.0000001 ## P-B -1.278186 -2.493918 -0.06245338 0.0367906 The output of the TukeyHSD’s test presents adjusted p-values (p adj) for the two groups comparisons. In addition is displayed the difference in means (diff) and the lower (lwr) and upper (upr) bounds of the 95% CI on the difference in means. In example 5, Owing the TukeyHSD’s test, it seems that all means are differ from one another. That is if we trust the appropriate use of the ANOVA and TukeyHSD’s tests… The ANOVA analysis relies on several assumptions that need to be tested before computing the ANOVA. The ANOVA formula is a based on mean and variance that can only be used if the distributions are not too different from the Normal distribution. It is a parametric test (see section 5.7). Before the ANOVA we need to verify with prior statistical tests that: the outcome variable should be normally distributed within each group (Shapiro test) the variance in each group should be similar (e.g. Bartlett or Levene test) the observations are independent (not correlated or related to each other) However, the F-test is fairly resistant or robust to violations of assumptions 1 and 2. 5.7 Parametric and non-parametric test "],
["introduction-to-regression-modelling.html", "Chapter 6 Introduction to regression modelling 6.1 Simple linear regression 6.2 Multiple linear regression", " Chapter 6 Introduction to regression modelling the step by step procedure to obtain an informative linear model, assess its validity and interpret it Interpret results of a statistical test Discuss significance of a statistical test 6.1 Simple linear regression 6.2 Multiple linear regression "],
["glossary.html", "Chapter 7 Glossary", " Chapter 7 Glossary "],
["references.html", "References", " References "]
]
